{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goethe University\n",
    "# Data Science and Marketing Analysis, Yelp Dataset <a name='t'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Libraries <a name='p1s1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import numpy as np\n",
    "import re\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Check-in <a name='p1h1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Reason starting with Check-in is to filter our dataset to only 1 year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Read & Normalize File <a name='p1h1s1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read & Normalize File because it was in Json format\n",
    "check = pd.read_csv('check_az_open.csv') #Original File\n",
    "check = pd.json_normalize(check.j.apply(json.loads))\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2- Create Every Unique Day As A List <a name='p1h1s2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To analyze Check-in Numbers Daily, we should get check-in numbers in tabular format for each businesses.\n",
    "\n",
    "#(~10 Minutes to Run)\n",
    "datelist = []\n",
    "for i in range(len(check)):\n",
    "    list_string2 = pd.Series(check['date'][i]).str.cat(sep=',')\n",
    "    list_split2 = list_string2.split(',') #211050\n",
    "    list_split_all2 = [x.strip(' ') for x in list_split2] #NoChange\n",
    "    datetime_list = []\n",
    "    for i in range(len(list_split_all2)):\n",
    "        datetime_list.append(datetime.strptime(list_split_all2[i], '%Y-%m-%d %H:%M:%S'))\n",
    "    date_list= []\n",
    "    for i in range(len(datetime_list)):\n",
    "        date_list.append(datetime.date(datetime_list[i]))\n",
    "    datelist.append(date_list)\n",
    "\n",
    "unique_datelist1 = []\n",
    "for i in datelist:\n",
    "    unique_datelist1.append(list(set(i)))\n",
    "\n",
    "unique_datelist2 = []\n",
    "for i in range(len(unique_datelist1)): #7230\n",
    "    for k in range(len(unique_datelist1[i])):\n",
    "        unique_datelist2.append(unique_datelist1[i][k])\n",
    "#\n",
    "unique_datelist2 = list(set(unique_datelist2))\n",
    "unique_datelist2.sort()\n",
    "for i in range(len(unique_datelist2)):\n",
    "    check[str(unique_datelist2[i])] = \"\"\n",
    "\n",
    "\"\"\"\n",
    "Date count for each business to columns\n",
    "\"\"\"\n",
    "for i in range(len(datelist)):\n",
    "    for date in datelist[i]:\n",
    "            trh = date.strftime(\"%Y-%m-%d\")\n",
    "            count = datelist[i].count(date)\n",
    "            check.loc[i,trh] = int(count)\n",
    "\n",
    "for i in range(len(datelist)):\n",
    "    for date in datelist[i]:\n",
    "            trh = date.strftime(\"%Y-%m-%d\")\n",
    "            count = datelist[i].count(date)\n",
    "            check.loc[i,trh] = int(count)\n",
    "\n",
    "check.drop(columns='date',inplace=True)\n",
    "#Save This File as check_1_by_dates.csv\n",
    "#check.to_csv('check_1_by_dates.csv',index=False)\n",
    "\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3- Transpose for Daily #s by Business <a name='p1h1s3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transposing to Have Business ID's in Columns and Dates in Rows\n",
    "#Later we will transpose again to get Weekly&Monthly Numbers\n",
    "\n",
    "# Create check_t\n",
    "check= pd.read_csv('check_1_by_dates.csv')\n",
    "check.drop(columns='Unnamed: 0',inplace=True)\n",
    "check_t = check.T\n",
    "check_t.reset_index(inplace=True)\n",
    "check_t.columns = check_t.loc[0,:] #Change column names\n",
    "check_t.drop(0,inplace=True) # Drop first row\n",
    "check_t.reset_index(drop=True,inplace=True)\n",
    "check_t.rename(columns={'business_id':'Date'},inplace=True)\n",
    "check_t = check_t[:-1] #Drop Sum Row, Last\n",
    "check_t.sort_values('Date',inplace=True)\n",
    "check_t['Date'] =  pd.to_datetime(check_t['Date'])\n",
    "\n",
    "# Filter only for 2015\n",
    "start_date = pd.to_datetime('01/01/2015')\n",
    "end_date = pd.to_datetime('01/01/2016')\n",
    "check_t_2015 = check_t.loc[(check_t['Date'] >= start_date) & (check_t['Date'] < end_date)].sort_values('Date')\n",
    "check_t_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4- Weekly Check-ins <a name='p1h1s4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Create Weekly and Monthly Checkin Numbers as Sum\n",
    "\n",
    "#Create Weekly Checkin Numbers as Sum\n",
    "ch_t_2015_weekly= check_t_2015.groupby(pd.Grouper(key='Date',freq='W'),as_index=False).sum()\n",
    "ch_2015_Weekly =ch_t_2015_weekly.T\n",
    "ch_2015_Weekly = pd.DataFrame(ch_2015_Weekly)\n",
    "ch_2015_Weekly.columns += 1 # Add 1 to each week number, (to fix)\n",
    "ch_2015_Weekly.reset_index(inplace=True)\n",
    "ch_2015_Weekly['sum'] = np.sum(ch_2015_Weekly.iloc[:,1:],axis=1)\n",
    "ch_2015_Weekly.rename(columns={0:'business_id'},inplace=True)\n",
    "\n",
    "#Save This file as check_2_2015_Weekly.csv\n",
    "#ch_2015_Weekly.to_csv('check_2_2015_Weekly.csv')\n",
    "ch_2015_Weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Monthly Check-ins <a name='p1h1s5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Monthly Checkin Numbers as Sum\n",
    "ch_2015_monthly_1 = check_t_2015.groupby(pd.Grouper(key='Date',freq='M'),as_index=False).sum()\n",
    "ch_2015_Monthly = ch_2015_monthly_1.T\n",
    "ch_2015_Monthly.columns += 1 # Add 1 to each month number, (to fix)\n",
    "ch_2015_Monthly['sum'] = np.sum(ch_2015_Monthly,axis=1)\n",
    "ch_2015_Monthly.reset_index(inplace=True)\n",
    "ch_2015_Monthly.rename(columns={0:'business_id'},inplace=True)\n",
    "\n",
    "#Save This file as check_3_2015_Monthly.csv\n",
    "#ch_2015_Monthly.to_csv('check_3_2015_Monthly.csv')\n",
    "\n",
    "ch_2015_Monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6- Filtering Businesses At Least 1 Monthly Check-in <a name='p1h1s6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Filter out Business that does not have at least #1 Check-in in each month in 2015, also create b_id_NZ_df\n",
    "ch_2015_Monthly_NZ = ch_2015_Monthly.loc[(ch_2015_Monthly[1] != 0) \n",
    "& (ch_2015_Monthly[2] != 0)\n",
    "& (ch_2015_Monthly[3] != 0)\n",
    "& (ch_2015_Monthly[4] != 0)\n",
    "& (ch_2015_Monthly[5] != 0)\n",
    "& (ch_2015_Monthly[6] != 0)\n",
    "& (ch_2015_Monthly[7] != 0)\n",
    "& (ch_2015_Monthly[8] != 0)\n",
    "& (ch_2015_Monthly[9] != 0)\n",
    "& (ch_2015_Monthly[10] != 0)\n",
    "& (ch_2015_Monthly[11] != 0)\n",
    "& (ch_2015_Monthly[12] != 0)\n",
    "]\n",
    "ch_2015_Monthly_NZ.reset_index(drop=True,inplace=True)\n",
    "ch_2015_Monthly_NZ.rename(columns={0:'business_id'},inplace=True)\n",
    "\n",
    "#Save This file as check_4_2015_Monthly_NZ.csv\n",
    "#ch_2015_Monthly_NZ.to_csv('check_4_2015_Monthly_NZ.csv')\n",
    "\n",
    "#2\n",
    "#Create and Save business_id's in check_4_2015_Monthly_NZ.csv\n",
    "#Also Add Business Numbers from 0, reason is having business_ID's in numbered format\n",
    "\n",
    "b_id_NZ_df = ch_2015_Monthly_NZ['business_id'].to_frame()\n",
    "b_id_NZ_df['b_num'] = b_id_NZ_df.index\n",
    "\n",
    "#Save This file as b_id_NZ_df.csv\n",
    "#b_id_NZ_df.to_csv('b_id_NZ_df.csv')\n",
    "ch_2015_Monthly_NZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7- Total # of CH for Each Day <a name='p1h1s7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see # of CH for Every Day in the Dataset\n",
    "\n",
    "check_dates = pd.DataFrame(check.apply(lambda x: pd.to_numeric(x,errors='coerce')).sum(axis=0).round(3))\n",
    "check_dates.reset_index(inplace=True)\n",
    "check_dates.rename(columns={'index':'date',0:'count'},inplace=True)\n",
    "check_dates.drop(check_dates.index[[[0,1,len(check_dates)-1]]],inplace=True)\n",
    "check_dates.reset_index(drop=True,inplace=True)\n",
    "check_dates['date'] = pd.to_datetime(check_dates.date)\n",
    "check_dates['WeekDay']= check_dates.date.dt.day_name()\n",
    "#Save This file as check_5_dates.csv\n",
    "#check_dates.to_csv('check_5_dates.csv')\n",
    "check_dates.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8- Create Check dates to Plot <a name='p1h1s8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Check Dates by Month\n",
    "ch_bymonth = check_dates.groupby(pd.Grouper(key='date',freq='M')).sum() #Dates Group by Month\n",
    "ch_bymonth.reset_index(inplace=True)\n",
    "ch_bymonth.rename(columns={'count':'sum'},inplace=True)\n",
    "ch_bymonth['month']=pd.to_datetime(ch_bymonth['date']).dt.strftime('%b-%Y')\n",
    "\n",
    "# Create Check Dates by Year\n",
    "ch_byyear = check_dates.groupby(pd.Grouper(key='date',freq='Y')).sum() #Dates Group by Year\n",
    "ch_byyear.reset_index(inplace=True)\n",
    "ch_byyear.rename(columns={'count':'sum'},inplace=True)\n",
    "ch_byyear['year']=pd.to_datetime(ch_byyear['date']).dt.strftime('%Y')\n",
    "\n",
    "#Check dates by Weekday\n",
    "ch_byweekday = check_dates.groupby('WeekDay').agg({       #Dates Group By Weekday\n",
    "'count': ['sum','mean','min','max']\n",
    "})\n",
    "ch_byweekday.reset_index(inplace=True)\n",
    "#Create days of week as a list\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "\n",
    "#Make ch_byweekday a single level column\n",
    "ch_byweekday.columns = ['WeekDay','sum','mean','min','max']\n",
    "\n",
    "ch_byweekday.set_index('WeekDay',inplace=True)\n",
    "ch_byweekday['WeekDay'] = days\n",
    "ch_byweekday = ch_byweekday.reindex(days)\n",
    "ch_byweekday.drop('WeekDay',axis=1,inplace=True)\n",
    "ch_byweekday.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9- Plot Checkin By WeekDay and Year <a name='p1h1s9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "#Check-in Based on Weekdays\n",
    "ax0 = ch_byweekday.plot(kind='bar',x='WeekDay',y='sum',ax=axs[0],color='lightcoral',rot=25,width=0.85)\n",
    "for container in ax0.containers:\n",
    "    ax0.bar_label(container,label_type='center',color='white',weight='bold')\n",
    "ax0.set_title('Total Check-in Based on Weekdays')\n",
    "#ax1 = ch_bymonth.plot(kind='bar',x='month',y='sum',ax=axs[1,[0,1]],color='r')\n",
    "ax0.set(xlabel='')\n",
    "ax2 = ch_byyear.plot(kind='bar',x='year',y='sum',ax=axs[1],color='lightcoral',width=0.9,rot=0)\n",
    "for container in ax2.containers:\n",
    "    ax2.bar_label(container,label_type='center',color='white',weight='bold',rotation=45)\n",
    "ax2.set_title('Total Check-in Based on Years')\n",
    "ax2.set(xlabel='')\n",
    "ax_list = [ax0,ax2]\n",
    "for ax in ax_list:\n",
    "    ax.yaxis.grid(color='silver',which='major',linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10- Create Check-in Frequency <a name='p1h1s10'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_freq = check_t_2015.T\n",
    "\n",
    "ch_freq.reset_index(inplace=True)\n",
    "\n",
    "for i in range(1,len(ch_freq.columns)):\n",
    "    ch_freq.iloc[0,i] = ch_freq.iloc[0,i].date()\n",
    "\n",
    "ch_freq.columns = ch_freq.iloc[0,:]\n",
    "\n",
    "ch_freq.drop(0,axis=0,inplace=True)\n",
    "ch_freq.rename(columns={'Date':'business_id'},inplace=True)\n",
    "ch_freq.replace(ch_freq.iloc[0,1],0,inplace=True)\n",
    "ch_freq['ch_freq'] = (np.count_nonzero(ch_freq,axis=1) / (len(ch_freq.columns)-1) )\n",
    "\n",
    "ch_freq = ch_freq[['business_id','ch_freq']]\n",
    "#Save ch_freq as ch_freq.csv\n",
    "#ch_freq.to_csv('check_6_freq.csv')\n",
    "\n",
    "#Create ch_freq 2015, also with b_num\n",
    "ch_freq_2015 = ch_freq.merge(b_id_NZ_df)\n",
    "#Save ch_freq_2015 as check_7_freq_2015.csv\n",
    "#ch_freq_2015.to_csv('check_7_freq_2015.csv')\n",
    "ch_freq_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read & Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Read Original File & Normalize\n",
    "bus = pd.read_csv('bus_az_open.csv')\n",
    "bus = pd.json_normalize(bus.j.apply(json.loads))\n",
    "bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Add Chain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whether Restaurant is A Chain or Not\n",
    "name_value_count = bus.name.value_counts().to_frame().reset_index().rename(columns={'name':'count','index':'name',})\n",
    "name_value_count.sample(5)\n",
    "bus = pd.merge(bus, name_value_count, on='name')\n",
    "for i in range(len(bus)):\n",
    "    if bus.loc[i,'count'] > 1:\n",
    "        bus.loc[i,'is_chain'] = 1\n",
    "    else:\n",
    "        bus.loc[i,'is_chain'] = 0\n",
    "bus.drop(columns='count',inplace=True)\n",
    "bus.loc[:,['name','is_chain']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filter only for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter only for 2015\n",
    "b_id_NZ_df = pd.read_csv('b_id_NZ_df.csv')\n",
    "b_id_NZ_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "bus = bus.merge(b_id_NZ_df, on='business_id')\n",
    "\n",
    "#Save This File as bus_1_2015.csv\n",
    "#bus.to_csv('bus_1_2015.csv', index=False)\n",
    "bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Distance to Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 Distance to City Center\n",
    "def haversine_vectorize(lon1, lat1, lon2, lat2):\n",
    "\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1\n",
    "\n",
    "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
    "\n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
    "    km = 6367 * dist #6367 for distance in KM for miles use 3958\n",
    "    return km\n",
    "                   \n",
    "bus['haversine_dist'] = haversine_vectorize(bus['longitude'],bus['latitude'],-112.0752558099556,33.44841768631837)\n",
    "bus.loc[:,['business_id','haversine_dist']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Add Income & Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income and Population Data for 2015\n",
    "#Source for income is http://www.usa.com/rank/arizona-state--per-capita-income--zip-code-rank.htm?yr=9000&dis=&wist=&plow=&phigh=\n",
    "income = pd.read_csv('Income_Az.csv')\n",
    "income.reset_index(drop=True,inplace=True)\n",
    "income['zip'] = income['zip'].astype(int, errors = 'raise')\n",
    "bus.rename(columns={'postal_code':'zip'},inplace=True)\n",
    "bus['zip'] = bus['zip'].astype(int, errors = 'ignore')\n",
    "bus = pd.merge(bus, income ,on= 'zip',how='left')\n",
    "\n",
    "#No Population & income info for 5 restaurants, so drop them\n",
    "bus.drop(bus[bus.population.isnull()].index,inplace=True)\n",
    "bus.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Since we dropped 5 businesses, also drop them from business_id_NZ_df, and save it as b_id_NZ_df_2.csv\n",
    "b_id_NZ_df_2 = bus.loc[:,['business_id','b_num']]\n",
    "b_id_NZ_df_2.reset_index(drop=True,inplace=True)\n",
    "b_id_NZ_df_2.to_csv('b_id_NZ_df_2.csv')\n",
    "\n",
    "bus.loc[:,['business_id','zip','pc_income']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Attributes Fix for Parking & Ambiance & GoodForMeal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Att_Fix(data,attribute_name):\n",
    "    data[attribute_name] = data[attribute_name].astype('str')\n",
    "    data[attribute_name] = data[attribute_name].replace({'\\'': '\"'},regex=True)\n",
    "    att_list = []\n",
    "    value_list = []\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(data)):\n",
    "        df.loc[i,'business_id'] = data.loc[i,'business_id']\n",
    "        pos_colon = [m.start() for m in re.finditer(':',data.loc[i,attribute_name])]\n",
    "        pos_dq = [m.start() for m in re.finditer('\"',data.loc[i,attribute_name])]\n",
    "        pos_comma = [m.start() for m in re.finditer(',',data.loc[i,attribute_name])]\n",
    "        pos_comma.append(len(data.loc[i,attribute_name])-1)\n",
    "        for j in range(len(pos_colon)):\n",
    "            j1 = j*2\n",
    "            j2 = (j*2 + 1)\n",
    "            att = data.loc[i,attribute_name][pos_dq[j1]+1:pos_dq[j2]]\n",
    "            att_list.append(att)\n",
    "\n",
    "            value = data.loc[i,attribute_name][pos_colon[j]+2:pos_comma[j]]\n",
    "            value_list.append(value)\n",
    "            pos1 = attribute_name.find('.')\n",
    "            attribute_name_2 = attribute_name[pos1+1:]\n",
    "            att_name = str(attribute_name_2) + '_' + att\n",
    "            df.loc[i,att_name] = value\n",
    "\n",
    "        df = df.fillna(0)\n",
    "        df.replace({False: 0, True: 1,'False':0, 'True':1, 'None':0},inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "parking_df = Att_Fix(bus,'attributes.BusinessParking')\n",
    "bus = pd.merge(bus,parking_df, on='business_id')\n",
    "ambiance_df = Att_Fix(bus,'attributes.Ambience')\n",
    "bus = pd.merge(bus,ambiance_df, on='business_id')\n",
    "meal_df = Att_Fix(bus,'GoodForMeal')\n",
    "bus = pd.merge(bus,meal_df, on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check BusinessParking\n",
    "bus.iloc[:,64:69].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Ambiance\n",
    "bus.iloc[:,70:77].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check GoodForMeal\n",
    "bus.iloc[:,-6:].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Wifi & Alcohol Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix Wifi Column and Preview\n",
    "bus['attributes.WiFi'].replace({\n",
    "\"u'free'\": 1,\n",
    "\"u'no'\": 0,\n",
    "\"'free'\":1,\n",
    "\"'no'\":0,\n",
    "\"u'paid'\":1,\n",
    "\"'paid'\":1,\n",
    "'None':0},inplace=True)\n",
    "\n",
    "bus['attributes.Alcohol'].replace({\n",
    "\"'beer_and_wine'\": 1,\n",
    "\"u'no'\": 0,\n",
    "\"'full_bar'\":1,\n",
    "\"'no'\":0,\n",
    "\"u'full_bar'\":1,\n",
    "\"u'beer_and_wine'\":1,\n",
    "'None':0,\n",
    "\"'none'\":0,\n",
    "\"u'none'\":0},inplace=True)\n",
    "\n",
    "bus.loc[:,['business_id','attributes.WiFi']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Categories List to Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Business Categories \n",
    "list_string = bus['categories'].str.cat(sep=',')\n",
    "list_split = list_string.split(',') #211050\n",
    "list_split_all = [x.strip(' ') for x in list_split] # Take Spaces\n",
    "count = Counter(list_split_all)\n",
    "unique_list = list(count.keys())\n",
    "most_frequent = count.most_common(20) #Change i to get i different categories\n",
    "bus_cat_unique = pd.DataFrame(most_frequent,columns=['Categories','count'])\n",
    "bus_cat_unique = bus_cat_unique[bus_cat_unique['Categories'] != 'Restaurants']\n",
    "bus_cat_unique.reset_index(inplace=True,drop='index')\n",
    "bus_cat_unique.sort_values('count',inplace=True,ascending=False)\n",
    "bus_cat_unique.drop(index=bus_cat_unique.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "bus_cat_unique.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#Categories List to Columns\n",
    "cat_list = bus_cat_unique.iloc[:,0].to_list()\n",
    "\n",
    "for i in range(len(bus)):\n",
    "    for j in cat_list:\n",
    "        name = str('c_') + j\n",
    "        if j in bus.iloc[i,8]:\n",
    "            bus.loc[i,name] = 1\n",
    "        else:\n",
    "            bus.loc[i,name] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Fix City Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make All Start with Capital Letter\n",
    "for i in range(len(bus)):\n",
    "    bus.loc[i,'city'] = bus.loc[i,'city'].title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Additional Changes for Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Changes for Attributes\n",
    "bus.drop(columns=['','Ambience','BusinessParking','RestaurantsAttire'],inplace=True)\n",
    "\n",
    "bus.rename(columns={'_breakfast':'GFM_breakfast',\n",
    " '_brunch':'GFM_brunch',\n",
    " '_dessert':'GFM_dessert',\n",
    " '_dinner':'GFM_dinner',\n",
    " '_latenight':'GFM_latenight',\n",
    " '_lunch':'GFM_lunch'},inplace=True)\n",
    "\n",
    "#bus.to_csv('bus_3_2015.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Photo Compliment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_comp = pd.read_csv('photo_az_compliment.csv')\n",
    "photo_comp.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "photo_comp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Photo Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, filter to 2015 and Preview Photo Table\n",
    "photo = pd.read_csv('photo_az_open.csv')\n",
    "photo.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "#Filter it for 2015, and save as photo_2015.csv\n",
    "photo_2015_NZ = photo.merge(b_id_NZ_df_2, on='business_id')\n",
    "photo_2015_NZ.to_csv('photo_2015_NZ.csv',index=False)\n",
    "photo_2015_NZ.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Review & Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Review - Read and Filter for 2015 & NZ Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv('review_az_open.csv')\n",
    "review.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "#After Read it, first for 2015, and then Filter for Monthly NZ businesses, \n",
    "\n",
    "start_date = pd.to_datetime('01/01/2015')\n",
    "end_date = pd.to_datetime('01/01/2016')\n",
    "review['date'] = pd.to_datetime(review['date']).dt.normalize()\n",
    "review_2015 = review.loc[(review['date'] >= start_date) & (review['date'] < end_date)].sort_values('date')\n",
    "review_2015.reset_index(drop=True,inplace=True)\n",
    "\n",
    "b_id_NZ_df_2 = pd.read_csv('b_id_NZ_df_2.csv')\n",
    "b_id_NZ_df_2.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "review_2015_NZ = pd.merge(review_2015, b_id_NZ_df_2 ,on= 'business_id')\n",
    "#Save This as review_2015_NZ.csv\n",
    "#review_2015_NZ.to_csv('review_2015_NZ.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview All Reviews (before Filter)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview Reviews for 2015 NZ businesses, almost 10% of All Reviews\n",
    "review_2015_NZ.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Create b_id_NZ_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Unique Businesses dropped to 1793, so save them as b_id_NZ_df_3.csv\n",
    "\n",
    "#Add Unique values of business_id in review_nz_2015 to a new dataframe\n",
    "b_id_nz_3_df = pd.DataFrame(list(review_2015_NZ['business_id'].unique()))\n",
    "b_id_nz_3_df.rename(columns={0:'business_id'},inplace=True)\n",
    "\n",
    "b_id_nz_3_df = b_id_nz_3_df.merge(review_2015_NZ[['business_id','b_num']],on='business_id',how='left')\n",
    "#Drop Duplicates\n",
    "b_id_nz_3_df.drop_duplicates(inplace=True)\n",
    "b_id_nz_3_df.reset_index(drop=True,inplace=True)\n",
    "#b_id_nz_3_df.to_csv('b_id_NZ_df_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Read Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('users_az_open.csv')\n",
    "users.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Add Gender for Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gender for All & Show Value Count\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "users.loc[:,'gender'] = users.loc[:,'name'].map(lambda x: d.get_gender(x))\n",
    "users.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Genders to Categorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.replace({'female':'F','male':'M','mostly_female':'F','mostly_male':'M','andy':'unknown'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Filter Users for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since our analysis on 2015, only works with users who starts yelping before 2015.\n",
    "end_date = pd.to_datetime('01/01/2015')\n",
    "users['yelping_since'] = pd.to_datetime(users['yelping_since']).dt.normalize()\n",
    "users_2015 = users.loc[(users['yelping_since'] < end_date)]\n",
    "#users_2015.to_csv('users_2015.csv')\n",
    "users_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = users_2015.groupby('gender').size().plot(kind='bar',rot=0,color=['lightcoral','grey','#145369'],xlabel='')\n",
    "for container in ax0.containers:\n",
    "    ax0.bar_label(container,label_type='center',color='white',weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Add User's Number of Rev in 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_rev_count_2015 = review_2015_NZ.groupby('user_id').size().to_frame().reset_index().rename(columns={0:'u_review_count_2015'})\n",
    "users_2015 = users_2015.merge(u_rev_count_2015,on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Create NZ User List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Users that have at least 1 fans, funny, cool, useful, and review \n",
    "users_2015_NZ = users_2015[(users_2015['fans'] != 0)\n",
    "& (users_2015['funny_count'] != 0)\n",
    "& (users_2015['cool_count'] != 0)\n",
    "& (users_2015['useful_count'] != 0)\n",
    "& (users_2015['friends_count'] != 0)\n",
    "& (users_2015['review_count'] != 0)]\n",
    "users_2015_NZ.reset_index(drop=True,inplace=True)\n",
    "users_2015_NZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Merge User and Review Files, for 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_user_2015 = pd.merge(review_2015_NZ, users_2015, on='user_id')\n",
    "review_user_2015.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "review_user_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Create b_id_NZ_df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Unique Businesses dropped to 1790, so save them as b_id_NZ_df_4.csv\n",
    "\n",
    "#Add Unique values of business_id in review_nz_2015 to a new dataframe\n",
    "b_id_nz_4_df = pd.DataFrame(list(review_user_2015['business_id'].unique()))\n",
    "b_id_nz_4_df.rename(columns={0:'business_id'},inplace=True)\n",
    "\n",
    "b_id_nz_4_df = b_id_nz_4_df.merge(review_user_2015[['business_id','b_num']],on='business_id',how='left')\n",
    "#Drop Duplicates\n",
    "b_id_nz_4_df.drop_duplicates(inplace=True)\n",
    "b_id_nz_4_df.reset_index(drop=True,inplace=True)\n",
    "#b_id_nz_4_df.to_csv('b_id_NZ_df_4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Sentiment Analysis on Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "my_stop_words = set(stopwords.words('english') + list(_stop_words.ENGLISH_STOP_WORDS) + ['super', 'duper', 'place'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# concatenate all the reviews into one single string \n",
    "full_text = ' '.join(review_user_2015.loc[100001:,'text'])\n",
    "#cloud_no_stopword = WordCloud(background_color='white', stopwords=my_stop_words).generate(full_text)\n",
    "\n",
    "# Important to select data to use in analysis\n",
    "data_to_use = review_user_2015.loc[:,'text']\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "lower_full_text = full_text.lower()\n",
    "word_tokens = word_tokenize(lower_full_text)\n",
    "tokens = list()\n",
    "for word in word_tokens:\n",
    "    if word.isalpha() and word not in my_stop_words:\n",
    "        tokens.append(word)\n",
    "token_dist = FreqDist(tokens)\n",
    "dist = pd.DataFrame(token_dist.most_common(20),columns=['Word', 'Frequency'])\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens =[porter.stem(word) for word in tokens]\n",
    "stemmed_token_dist = FreqDist(stemmed_tokens)\n",
    "stemmed_dist = pd.DataFrame(stemmed_token_dist.most_common(20),columns=['Word', 'Frequency'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words, ngram_range=(2,2))\n",
    "bigrams = vect.fit_transform(data_to_use)\n",
    "bigram_df = pd.DataFrame(bigrams.toarray(), columns=vect.get_feature_names())\n",
    "bigram_frequency = pd.DataFrame(bigram_df.sum(axis=0)).reset_index()\n",
    "bigram_frequency.columns = ['bigram', 'frequency']\n",
    "bigram_frequency = bigram_frequency.sort_values(by='frequency', ascending=False).head(20)\n",
    "\n",
    "# Load SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = data_to_use.apply(sid.polarity_scores)\n",
    "#review_2015_NZ = review_2015_NZ.set_index('date')\n",
    "review_user_2015.loc[:,'sentiment_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "#monthly_sentiment = sentiment.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New Dataframe review_user_2015_sent, with sentiment scores\n",
    "review_user_2015_sent = review_user_2015.copy()\n",
    "\n",
    "#Add rev_id for each review, and save DataFrame XX\n",
    "review_user_2015_sent.reset_index(drop=True,inplace=True)\n",
    "review_user_2015_sent['rev_id'] = review_user_2015_sent.index\n",
    "#review_user_2015_sent.to_csv('review_user_2015_sent.csv',index=False)\n",
    "\n",
    "#Also save rev_id and sentiment score only DataFrame\n",
    "review_user_2015_sent[['rev_id','sentiment_score']].to_csv('review_sentscore_revid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Rev_Users AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Fixes on Review_User File\n",
    "\n",
    "#1 Add How Many month that users have been yelping\n",
    "review_user_2015_sent['yelping_since'] = pd.to_datetime(review_user_2015_sent['yelping_since'])\n",
    "review_user_2015_sent['date'] = pd.to_datetime(review_user_2015_sent['date'])\n",
    "\n",
    "review_user_2015_sent['yelping_month'] = review_user_2015_sent['date'].dt.to_period('M').astype(int) - \\\n",
    "    review_user_2015_sent['yelping_since'].dt.to_period('M').astype(int)\n",
    "\n",
    "#2 Add Whether Users is Elite or not in 2015\n",
    "review_user_2015_sent['elite'] = review_user_2015_sent['elite'].astype(str)\n",
    "\n",
    "for i in range(len(review_user_2015_sent)):\n",
    "    if '2015' in review_user_2015_sent.loc[i,'elite']:\n",
    "        review_user_2015_sent.loc[i,'is_elite_2015'] = 1\n",
    "    else:\n",
    "        review_user_2015_sent.loc[i,'is_elite_2015'] = 0\n",
    "\n",
    "#3 Replace Naming for Genders\n",
    "review_user_2015_sent.replace({'female':'F','male':'M','mostly_female':'F','mostly_male':'F','andy':'nan','unknown':'nan'},inplace=True)\n",
    "\n",
    "#4 Add True Star, which is difference of star on review and avg. star of that user.\n",
    "review_user_2015_sent['true_star'] = review_user_2015_sent['stars'] - review_user_2015_sent['avg_stars']\n",
    "\n",
    "\n",
    "#5 Get Mean Values for each business in each features, with avg. user values\n",
    "rev_us_2015_avg = review_user_2015_sent.groupby('business_id').agg({'text':'count',\n",
    "'cool':'mean',\n",
    "'useful':'mean',\n",
    "'stars':'mean',\n",
    "'funny':'mean',\n",
    "'review_count':'mean',\n",
    "'friends_count':'mean',\n",
    "'fans':'mean',\n",
    "'true_star':'mean',\n",
    "'avg_stars':'mean',\n",
    "'sentiment_score':'mean',\n",
    "'is_elite_2015':'mean',\n",
    "'yelping_month':'mean',\n",
    "'u_review_count_2015':'mean'})\n",
    "rev_us_2015_avg.reset_index(inplace=True)\n",
    "rev_us_2015_avg.rename(columns={'text':'rev_count',\n",
    "'cool':'r_avg_cool',\n",
    "'useful':'r_avg_useful',\n",
    "'stars':'r_avg_stars',\n",
    "'funny':'r_avg_funny',\n",
    "'review_count':'u_avg_rev_count',\n",
    "'friends_count':'u_avg_friend',\n",
    "'fans':'u_avg_fans',\n",
    "'true_star':'u_avg_true_star',\n",
    "'avg_stars':'u_avg_avg_star',\n",
    "'sentiment_score':'avg_sentiment_score',\n",
    "'is_elite_2015':'u_avg_is_elite_2015',\n",
    "'yelping_month':'u_avg_yelping_month'},inplace=True)\n",
    "\n",
    "#6 Get Gender Count And Ratio \n",
    "rev_us_gend_count_m = pd.DataFrame(review_user_2015_sent.groupby('business_id').gender.apply(lambda x: x[x=='M'].count()))\n",
    "rev_us_gend_count_f = pd.DataFrame(review_user_2015_sent.groupby('business_id').gender.apply(lambda x: x[x=='F'].count()))\n",
    "rev_us_gend_count_m.reset_index(inplace=True)\n",
    "rev_us_gend_count_f.reset_index(inplace=True)\n",
    "rev_us_gend_count_m.rename(columns={'gender':'M_count'},inplace=True)\n",
    "rev_us_gend_count_f.rename(columns={'gender':'F_count'},inplace=True)\n",
    "rev_us_gend_count = pd.merge(rev_us_gend_count_f,rev_us_gend_count_m,on='business_id')\n",
    "rev_us_gend_count['M/F Ratio'] = rev_us_gend_count['M_count'] / rev_us_gend_count['F_count']\n",
    "rev_us_gend_count['M/F Ratio'] = rev_us_gend_count['M/F Ratio'].astype(float)\n",
    "rev_us_gend_count = rev_us_gend_count.replace({np.inf:'0'})\n",
    "\n",
    "#7 Merge With Rev-User File\n",
    "rev_us_2015_avg = pd.merge(rev_us_2015_avg,rev_us_gend_count,on='business_id')\n",
    "rev_us_2015_avg.drop(columns=['F_count','M_count'],inplace=True)\n",
    "\n",
    "#8 Add Check Frequencies\n",
    "rev_us_2015_avg = pd.merge(rev_us_2015_avg,ch_freq_2015,on='business_id')\n",
    "rev_us_2015_avg['M/F Ratio'] = rev_us_2015_avg['M/F Ratio'].astype(float)\n",
    "\n",
    "#Save Avg File as review_user_2015_avg.csv\n",
    "rev_us_2015_avg.to_csv('review_user_2015_avg.csv',index=False)\n",
    "\n",
    "rev_us_2015_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Daily Review Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_daily = review_user_2015_sent[['date','business_id']]\n",
    "review_daily.date = review_daily.apply(lambda x: datetime.date(x['date']),axis=1)\n",
    "review_daily = review_daily.groupby(['business_id','date']).size().unstack().reset_index()\n",
    "#Save Review_Daily as review_daily.csv\n",
    "review_daily.to_csv('review_daily.csv',index=False)\n",
    "\n",
    "review_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip = pd.read_csv('tip_az_open2.csv') #Previous one doesn't have dates\n",
    "tip.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter it for 2015 NZ, and preview. %43 Decrease\n",
    "\n",
    "start_date = pd.to_datetime('01/01/2015')\n",
    "end_date = pd.to_datetime('01/01/2016')\n",
    "tip['date'] = pd.to_datetime(review['date']).dt.normalize()\n",
    "tip_2015 = tip.loc[(tip['date'] >= start_date) & (tip['date'] < end_date)].sort_values('date')\n",
    "tip_2015.reset_index(drop=True,inplace=True)\n",
    "tip_2015_NZ = tip_2015.merge(b_id_nz_4_df, on='business_id', how='inner')\n",
    "\n",
    "tip_2015_NZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sentiment Analysis on Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "tip_2015_NZ['tip'] = [i.replace(\"&amp;amp;\", '').replace(\"\\'\",'') for i in tip_2015_NZ['tip']]\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "my_stop_words = set(stopwords.words('english') + list(_stop_words.ENGLISH_STOP_WORDS) + ['super', 'duper', 'place'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# concatenate all the reviews into one single string \n",
    "full_text = ' '.join(tip_2015_NZ.loc[:,'tip'])\n",
    "cloud_no_stopword = WordCloud(background_color='white', stopwords=my_stop_words).generate(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "lower_full_text = full_text.lower()\n",
    "word_tokens = word_tokenize(lower_full_text)\n",
    "tokens = list()\n",
    "for word in word_tokens:\n",
    "    if word.isalpha() and word not in my_stop_words:\n",
    "        tokens.append(word)\n",
    "token_dist = FreqDist(tokens)\n",
    "dist = pd.DataFrame(token_dist.most_common(20),columns=['Word', 'Frequency'])\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens =[porter.stem(word) for word in tokens]\n",
    "stemmed_token_dist = FreqDist(stemmed_tokens)\n",
    "stemmed_dist = pd.DataFrame(stemmed_token_dist.most_common(20),columns=['Word', 'Frequency'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words, ngram_range=(2,2))\n",
    "bigrams = vect.fit_transform(tip_2015_NZ.loc[:,'tip'])\n",
    "bigram_df = pd.DataFrame(bigrams.toarray(), columns=vect.get_feature_names())\n",
    "bigram_frequency = pd.DataFrame(bigram_df.sum(axis=0)).reset_index()\n",
    "bigram_frequency.columns = ['bigram', 'frequency']\n",
    "bigram_frequency = bigram_frequency.sort_values(by='frequency', ascending=False).head(20)\n",
    "\"\"\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "df_good = review_2015_NZ.loc[:5000][review_2015_NZ['stars'] >= 4]\n",
    "good_reviews = ' '.join(df_good.text)\n",
    "# split the long string into sentences\n",
    "sentences_good = sent_tokenize(good_reviews)\n",
    "good_token_clean = list()\n",
    "# get tokens for each sentence\n",
    "import re\n",
    "for sentence in sentences_good:\n",
    "    eng_word = re.findall(r'[A-Za-z\\-]+', sentence)\n",
    "    good_token_clean.append([i.lower() for i in eng_word if i.lower() not in my_stop_words])\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model_ted = Word2Vec(sentences=good_token_clean, size=500, window=10, min_count=1, workers=4, sg=0)\n",
    "model_ted.predict_output_word(['good'], topn=10)\n",
    "\"\"\"\n",
    "# Load SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = tip_2015_NZ.loc[:,'tip'].apply(sid.polarity_scores)\n",
    "#review_2015_NZ = review_2015_NZ.set_index('date')\n",
    "tip_2015_NZ.loc[:,'sentiment_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "#monthly_sentiment = sentiment.resample('M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add tip_id for each review, and save DataFrame\n",
    "tip_2015_NZ['tip_id'] = tip_2015_NZ.index\n",
    "tip_2015_NZ.to_csv('tip_2015_NZ.csv',index=False)\n",
    "\n",
    "#Also save tip_id and sentiment score only DataFrame\n",
    "tip_2015_NZ[['tip_id','sentiment_score']].to_csv('tip_sentscore_tipid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Daily Tip Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_daily = tip_2015_NZ[['date','business_id']]\n",
    "tip_daily.date = tip_daily.apply(lambda x: datetime.date(x['date']),axis=1)\n",
    "tip_daily = tip_daily.groupby(['business_id','date']).size().unstack().reset_index()\n",
    "#Save Review_Daily as review_daily.csv\n",
    "tip_daily.to_csv('tip_daily.csv',index=False)\n",
    "\n",
    "tip_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Descriptive Analysis <a name='p2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = pd.read_csv('bus_3_2015.csv')\n",
    "bus.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "\n",
    "b_id_NZ_df_4 = pd.read_csv('b_id_NZ_df_4.csv')\n",
    "b_id_NZ_df_4.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "\n",
    "#Even if b_id_NZ_df_5 will be created after the weather data drop,\n",
    "#it will be used in Descriptive Analysis\n",
    "b_id_NZ_df_5 = pd.read_csv('b_id_NZ_df_5.csv')\n",
    "b_id_NZ_df_5.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "\n",
    "bus = bus.merge(b_id_NZ_df_5, on=['business_id','b_num'], how='inner')\n",
    "bus.reset_index(inplace=True,drop=True)\n",
    "bus.columns = bus.columns.map(lambda x: x.removeprefix(\"attributes.\"))\n",
    "bus.columns = bus.columns.map(lambda x: x.removeprefix(\"GoodForMeal\"))\n",
    "\n",
    "#check\n",
    "check_dates = pd.read_csv('check_5_dates.csv')\n",
    "check_dates.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "check_dates.date = pd.to_datetime(check_dates.date)\n",
    "check_dates = check_dates.loc[check_dates.date.dt.year == 2015]\n",
    "\n",
    "#check_t_2015 = pd.read_csv('check_t_2015.csv')\n",
    "#check_t_2015.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "\n",
    "check_weekly = pd.read_csv('check_2_2015_Weekly.csv')\n",
    "check_weekly.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "check_weekly = check_weekly.merge(b_id_NZ_df_4, on='business_id', how='inner')\n",
    "\n",
    "check_monthly = pd.read_csv('check_3_2015_Monthly.csv')\n",
    "check_monthly.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "check_monthly = check_monthly.merge(b_id_NZ_df_4, on='business_id', how='inner')\n",
    "\n",
    "#photo\n",
    "photo = pd.read_csv('photo_2015_NZ.csv')\n",
    "photo.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "photo = photo.merge(b_id_NZ_df_4, on=['business_id','b_num'], how='inner')\n",
    "photo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "photo_comp = pd.read_csv('photo_comp_2015_NZ.csv')\n",
    "photo_comp.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "photo_comp = photo_comp.merge(b_id_NZ_df_4, on=['business_id','b_num'], how='inner')\n",
    "photo_comp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#review-user\n",
    "review_user = pd.read_csv('review_user_2015_sent.csv')\n",
    "review_user.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "review_user = review_user.merge(b_id_NZ_df_4, on=['business_id','b_num'], how='inner')\n",
    "review_user.reset_index(drop=True, inplace=True)\n",
    "\n",
    "review_avg = pd.read_csv('review_user_2015_avg.csv')\n",
    "review_avg.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "review_avg = review_avg.merge(b_id_NZ_df_4, on=['business_id','b_num'], how='inner')\n",
    "review_avg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#tips\n",
    "tips = pd.read_csv('tip_2015_NZ.csv') \n",
    "tips.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n",
    "tips = tips.merge(b_id_NZ_df_4, on=['business_id','b_num'], how='inner')\n",
    "tips.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ch_freq = pd.read_csv('check_7_freq_2015.csv')\n",
    "ch_freq.drop(columns='Unnamed: 0',inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Businesses in Each City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bus)):\n",
    "    bus.loc[i,'city'] = bus.loc[i,'city'].title()\n",
    "bus.city.replace({'Scottdale':'Scottsdale'},inplace=True)\n",
    "\n",
    "ax0 = bus.groupby('city').size().sort_values(ascending=False)[:10].plot(kind='bar',figsize=(10,5),rot=0,color='#145369')\n",
    "for container in ax0.containers:\n",
    "    ax0.bar_label(container, label_type='center', fontsize=13,color='white', fontweight='bold')\n",
    "ax0.set_title('Number of Restaurant in Top 10 Cities', fontsize=15, fontweight='bold')\n",
    "#plt.savefig('Number of Restaurant in Top 10 Cities.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ch_freq for each Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_c_c = round(bus.groupby('city')['ch_freq'].mean().sort_values(ascending=False),2).plot(kind='bar',figsize=(10,5),color='#145369')\n",
    "for container in ax_c_c.containers:\n",
    "    ax_c_c.bar_label(container, label_type='center', fontsize=10,color='white', fontweight='bold',rotation=90)\n",
    "ax_c_c.set_title('Average Check-in Frequency in Cities', fontsize=15, fontweight='bold')\n",
    "#plt.savefig('Average Check-in Frequency in Cities.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Categories Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Business Categories Plot \n",
    "list_string = bus['categories'].str.cat(sep=',')\n",
    "list_split = list_string.split(',') #211050\n",
    "list_split_all = [x.strip(' ') for x in list_split] # Take Spaces\n",
    "count = Counter(list_split_all)\n",
    "unique_list = list(count.keys())\n",
    "most_frequent = count.most_common(20) #Change i to get i different categories\n",
    "bus_cat_unique = pd.DataFrame(most_frequent,columns=['Categories','count'])\n",
    "bus_cat_unique = bus_cat_unique[bus_cat_unique['Categories'] != 'Restaurants']\n",
    "bus_cat_unique.reset_index(inplace=True,drop='index')\n",
    "bus_cat_unique.sort_values('count',inplace=True,ascending=False)\n",
    "bus_cat_unique.drop(index=bus_cat_unique.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "bus_cat_unique.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#Plot Categories\n",
    "ax0 = bus_cat_unique.plot(kind='bar', y='count',x='Categories',figsize=(13,5),color='#145369')\n",
    "for container in ax0.containers:\n",
    "        ax0.bar_label(container, label_type='center',rotation=90,color='white',weight='bold',fontsize=13)\n",
    "ax0.set_title('Number of Restaurants in Top 20 Categories', fontsize=15, fontweight='bold')\n",
    "#plt.savefig('Number of Restaurants in Top 20 Categories.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus.categories = bus.categories.str.replace(r\"(\",\"\")\n",
    "bus.categories = bus.categories.str.replace(r\")\",\"\")\n",
    "bus_cat_unique.Categories = bus_cat_unique.Categories.str.replace(r\"(\",\"\")\n",
    "bus_cat_unique.Categories = bus_cat_unique.Categories.str.replace(r\")\",\"\")\n",
    "cat_ch_df = pd.DataFrame(columns=['Categories','ch_freq'])\n",
    "for i in range(len(bus_cat_unique)):\n",
    "    cat_ch_df.loc[len(cat_ch_df),:] = [bus_cat_unique.iloc[i,0],bus[bus['categories'].str.contains(bus_cat_unique.iloc[i,0])]['ch_freq'].mean()]\n",
    "cat_ch_df.ch_freq = cat_ch_df.ch_freq.astype(float)\n",
    "cat_ch_df.ch_freq = round(cat_ch_df.ch_freq,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check-in Freq by Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_cat = cat_ch_df.sort_values('ch_freq',ascending=True).plot(kind='barh',y='ch_freq',x='Categories',figsize=(7,7),color='#145369')\n",
    "for container in ax_cat.containers:\n",
    "        ax_cat.bar_label(container, label_type='center',color='white',weight='bold')\n",
    "ax_cat.set_title('Average Check-in Frequency by Categories', fontsize=15, fontweight='bold')\n",
    "#plt.savefig('Average Check-in Frequency by Categories.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check-in by Price Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_c_pr = round(bus.groupby('RestaurantsPriceRange2')['ch_freq'].mean(),3).plot(kind='bar',rot=0,color='#145369')\n",
    "for container in ax_c_pr.containers:\n",
    "    ax_c_pr.bar_label(container, label_type='center',color='white',weight='bold')\n",
    "ax_c_pr.set_xlabel('Price Range')\n",
    "ax_c_pr.set_ylabel('Average Check Frequency')\n",
    "ax_c_pr.set_title('Average Check Frequency by Price Range',weight='bold')\n",
    "plt.savefig('Average Check Frequency by Price Range.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stars Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = bus.groupby('stars').size().plot(kind='bar',figsize=(5,4),color='#145369',rot=0,width=0.8)\n",
    "for container in ax1.containers:\n",
    "        ax1.bar_label(container, label_type='edge',color='lightcoral',weight='bold')\n",
    "ax1.set_title('Number of Restaurants by Star Rating', fontsize=13, fontweight='bold')\n",
    "#plt.savefig('Number of Restaurants by Star Rating.png',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check-ins by Day & Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(20,5))\n",
    "ch_byweekday = check_dates.groupby('WeekDay').sum()\n",
    "ch_byweekday.reset_index(inplace=True)\n",
    "#Create days of week as a list\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "\n",
    "#Make ch_byweekday a single level column\n",
    "ch_byweekday.columns = ['WeekDay','sum']\n",
    "\n",
    "ch_byweekday.set_index('WeekDay',inplace=True)\n",
    "ch_byweekday['WeekDay'] = days\n",
    "ch_byweekday = ch_byweekday.reindex(days)\n",
    "ch_byweekday.drop('WeekDay',axis=1,inplace=True)\n",
    "ch_byweekday.reset_index(inplace=True)\n",
    "\n",
    "ax2 = ch_byweekday.plot(kind='bar',color='black',x='WeekDay',y='sum',rot=0,ax=axs[1],width=0.65)\n",
    "for container in ax2.containers:\n",
    "        ax2.bar_label(container, label_type='center',rotation=0,color='white',weight='bold',size=13)\n",
    "ax2.set_title('Checkins by Day of Week', fontsize=20, fontweight='bold')\n",
    "ax2.set(xlabel='')\n",
    "#Create a list of the months\n",
    "months = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "ch_bymonth = check_dates.groupby(pd.Grouper(key='date', freq='M')).sum().reset_index()\n",
    "ch_bymonth['date'] = months\n",
    "ch_bymonth.rename(columns={'date':'Month','count':'sum'},inplace=True)\n",
    "ax3 = ch_bymonth.plot(kind='bar',color='black',x='Month',y='sum',rot=25,width=0.9,ax=axs[0])\n",
    "for container in ax3.containers:\n",
    "        ax3.bar_label(container, label_type='center',rotation=30,color='white',weight='bold',size=12)\n",
    "ax3.set_title('Checkins by Month', fontsize=20, fontweight='bold')\n",
    "ax3.set(xlabel='')\n",
    "plt.savefig('Checkins by Day of Week and Month_black.png',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds for Tips & Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "my_stop_words = set(stopwords.words('english') + list(_stop_words.ENGLISH_STOP_WORDS) + ['super', 'duper', 'place'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# concatenate all the reviews into one single string \n",
    "full_text = ' '.join(tips.loc[:,'tip'])\n",
    "cloud_no_stopword = WordCloud(background_color='white', stopwords=my_stop_words).generate(full_text)\n",
    "\n",
    "\n",
    "full_text_rew = ' '.join(review_user.loc[:,'text'])\n",
    "cloud_no_stopword2 = WordCloud(background_color='white', stopwords=my_stop_words).generate(full_text_rew)\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(1,2, figsize=(20,5))\n",
    "#Plot cloud_no_stopword and cloud_no_stopword2 in subplots\n",
    "ax1[0].imshow(cloud_no_stopword, interpolation='bilinear')\n",
    "ax1[0].axis('off')\n",
    "ax1[0].set_title('Word Cloud of Tips', fontsize=20, fontweight='bold')\n",
    "ax1[1].imshow(cloud_no_stopword2, interpolation='bilinear')\n",
    "ax1[1].axis('off')\n",
    "ax1[1].set_title('Word Cloud of Reviews', fontsize=20, fontweight='bold')\n",
    "#plt.savefig('Word Cloud of Tips and Reviews.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "mask = np.array(Image.open(path.join(d, \"yelp_logo_cmyk_2.png\")))\n",
    "full_text_2 = full_text + full_text_rew\n",
    "wc = WordCloud(max_words=1000, mask=mask, stopwords=my_stop_words).generate(full_text_2)\n",
    "# store default colored image\n",
    "#wc2 = WordCloud(max_words=1000, mask=mask, stopwords=my_stop_words).generate(full_text_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10),dpi=300)\n",
    "plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "#wc.to_file(\"a_new_hope.png\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Word Cloud of for Tips and Review_black.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Graphs for Tips & Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lineplot for sentiment score and stars in tip_2015_NZ with variance, with standart deviation\n",
    "sns.set(style='whitegrid')\n",
    "fig, ax3 = plt.subplots(1,2, figsize=(15,5))\n",
    "ax3_1 = sns.lineplot(x='stars', y='sentiment_score', data=tips ,err_style='band',ci='sd',ax=ax3[0])\n",
    "ax3_1.set_title('Sentiment Score vs Stars for Tips', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Create lineplot for sentiment score and stars in review_user_2015_sent with variance, with standart deviation\n",
    "ax3_2 = sns.lineplot(x='stars', y='sentiment_score', data=review_user,err_style='band',ci='sd',ax=ax3[1])\n",
    "ax3_2.set_title('Sentiment Score vs Stars for Reviews', fontsize=20, fontweight='bold')\n",
    "#plt.savefig('Sentiment Score vs Stars for Tips and Reviews.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Sentiment by Check-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips2 = tips.merge(ch_freq)\n",
    "rew2 = review_user.merge(ch_freq)\n",
    "\n",
    "#rew2.drop('rev_id',axis=1,inplace=True)\n",
    "#rew2.drop_duplicates(inplace=True)\n",
    "#rew2.reset_index(inplace=True,drop=True)\n",
    "\n",
    "rew_2_df = rew2.groupby('ch_freq')['sentiment_score'].mean().reset_index()\n",
    "tips_2_df = tips2.groupby('ch_freq')['sentiment_score'].mean().reset_index()\n",
    "# Create lineplot for sentiment score and stars in tip_2015_NZ with variance, with standart deviation\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set(style='whitegrid')\n",
    "ax3_1 = sns.regplot(x='ch_freq', y='sentiment_score',data=tips_2_df,scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\n",
    "ax3_1.set_title('Avg. Sentiment Score by Check-in Freq. for Tips', fontsize=15, fontweight='bold',color='black')\n",
    "#plt.show()\n",
    "plt.savefig('Avg. Sentiment Score by Check-in for Tips.png',dpi=300,bbox_inches='tight')\n",
    "# Create lineplot for sentiment score and stars in review_user_2015_sent with variance, with standart deviation\n",
    "plt.figure(figsize=(10,5))\n",
    "ax3_2 = sns.regplot(x='ch_freq', y='sentiment_score',data=rew_2_df,scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"})\n",
    "ax3_2.set_title('Avg. Sentiment Score by Check-in Freq. for Reviews', fontsize=15, fontweight='bold',color='black')\n",
    "#plt.show()\n",
    "plt.savefig('Avg. Sentiment Score by Check-in for Reviews.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rev_AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_avg = review_avg.merge(b_id_NZ_df_5, on=['business_id','b_num'], how='inner')\n",
    "plt.figure(figsize=(12,12))\n",
    "ax_avg = sns.heatmap(review_avg.corr(), annot=True, cmap='Greys',square=True, linecolor='white',annot_kws={\"size\": 10})\n",
    "ax_avg.set_title('Correlation Matrix of AVG Reviews', fontsize=20, fontweight='bold')\n",
    "#plt.savefig('Correlation Matrix of AVG Reviews.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rev_Avg to Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(review_avg.iloc[:,:-2].describe().T.iloc[:,[0,1,2,3,7]],3).to_latex().replace(\"\\\\\\n\", \"\\\\ \\hline\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income and Check-freq by ZipCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = bus[['zip','pc_income','ch_freq']].groupby('zip').mean().reset_index().plot(kind='scatter',x='zip',y='ch_freq')\n",
    "ax1 = ax0.twinx()\n",
    "bus[['zip','pc_income','ch_freq']].groupby('zip').mean().reset_index().plot(kind='scatter',x='zip',\n",
    "y='pc_income',ax=ax1,color='r',)\n",
    "ax1 = ax0.twinx()\n",
    "bus[['zip','pc_income','ch_freq']].groupby('zip').mean().reset_index().plot(kind='scatter',x='zip',\n",
    "y='pc_income',ax=ax1,color='r',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax_h = plt.hist(bus.pc_income)\n",
    "#Add Title and Labels\n",
    "#Add Density Plot\n",
    "ax_d = sns.distplot(bus.pc_income, bins=10, kde=True, color='black', hist_kws={'alpha':0.5})\n",
    "ax_d.set_xlabel('Income')\n",
    "ax_d.set_ylabel('Density')\n",
    "ax_d.set_title('Income Distribution')\n",
    "plt.savefig('Income_Dist.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_user.gender.replace(np.NaN,'Unknown',inplace=True)\n",
    "ax_g = review_user.groupby('gender').size().to_frame().reset_index().rename(columns={0:'count'}).plot(kind='bar',rot=0,x='gender',color='#145369')\n",
    "for container in ax_g.containers:\n",
    "    ax_g.bar_label(container,color='white',label_type='center',size=15,weight='bold')\n",
    "ax_g.set_xticklabels(['Female','Male','Unknown'])\n",
    "ax_g.set_title('Number of Reviews by Gender',size=13,weight='bold')\n",
    "plt.savefig('Number of Reviews by Gender',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap of Resturants Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = bus[['latitude', 'longitude']]\n",
    "gmaps.configure(api_key='*')\n",
    "\n",
    "figure_layout = {\n",
    "    'width': '500px',\n",
    "    'height': '400px',}\n",
    "figx = gmaps.figure()\n",
    "figx.add_layer(gmaps.heatmap_layer(locations))\n",
    "figx\n",
    "#plt.savefig('Heatmap of Business Locations.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income & Dist & Check Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dist and Check Freq\n",
    "bus.plot(kind='scatter',x='haversine_dist',y='ch_freq',color='#145369')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Count and Plot Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes List\n",
    "att_list1 = ['WiFi', 'HasTV',\n",
    "       'Alcohol', 'GoodForKids', 'OutdoorSeating', 'RestaurantsTakeOut',\n",
    "       'RestaurantsDelivery', \n",
    "       'RestaurantsReservations', 'RestaurantsGoodForGroups',\n",
    "       'BusinessAcceptsCreditCards', 'Caters', 'BikeParking',\n",
    "       'RestaurantsTableService',  'DriveThru', 'WheelchairAccessible',\n",
    "       'BusinessAcceptsBitcoin',   'CoatCheck',\n",
    "       'HappyHour', 'BestNights', 'DogsAllowed', 'GoodForDancing', \n",
    "        'ByAppointmentOnly', 'DietaryRestrictions', \n",
    "       'AgesAllowed', 'HairSpecializesIn', 'AcceptsInsurance',\n",
    "       'RestaurantsCounterService', 'Open24Hours', \n",
    "        'BusinessParking_garage',\n",
    "       'BusinessParking_street', 'BusinessParking_validated',\n",
    "       'BusinessParking_lot', 'BusinessParking_valet', 'Ambience_romantic',\n",
    "       'Ambience_intimate', 'Ambience_classy', 'Ambience_hipster',\n",
    "       'Ambience_divey', 'Ambience_touristy', 'Ambience_trendy',\n",
    "       'Ambience_upscale', 'Ambience_casual', 'GFM_dessert', 'GFM_latenight',\n",
    "       'GFM_lunch', 'GFM_dinner', 'GFM_brunch', 'GFM_breakfast', 'c_Nightlife',\n",
    "       'c_American (Traditional)', 'c_Bars', 'c_American (New)', 'c_Mexican',\n",
    "       'c_Breakfast & Brunch', 'c_Sandwiches', 'c_Burgers', 'c_Fast Food',\n",
    "       'c_Pizza', 'c_Italian', 'c_Salad', 'c_Seafood',\n",
    "       'c_Event Planning & Services', 'c_Chinese', 'c_Sports Bars',\n",
    "       'c_Sushi Bars', 'c_Coffee & Tea']\n",
    "\n",
    "att_list2 = ['WiFi', 'HasTV',\n",
    "       'Alcohol', 'GoodForKids', 'OutdoorSeating', 'RestaurantsTakeOut',\n",
    "       'RestaurantsDelivery', \n",
    "       'RestaurantsReservations', 'RestaurantsGoodForGroups',\n",
    "       'BusinessAcceptsCreditCards', 'BikeParking',\n",
    "       'RestaurantsTableService',  'DriveThru', 'WheelchairAccessible',\n",
    "       'BusinessAcceptsBitcoin',  'CoatCheck',\n",
    "       'HappyHour', 'DogsAllowed', 'GoodForDancing',\n",
    "        'ByAppointmentOnly', \n",
    "       'HairSpecializesIn', 'AcceptsInsurance',\n",
    "       'RestaurantsCounterService', 'Open24Hours']\n",
    "\n",
    "# Null Counter\n",
    "bus.replace({'1':1, '0':0}, inplace=True)\n",
    "b_null_count = pd.isnull(bus[att_list2])\n",
    "\n",
    "b_null_count\n",
    "# If isnull = False, value is 1, otherwise 0\n",
    "b_null_count.replace({False: 1, True: 0},inplace=True)\n",
    "b_null_count = b_null_count.append((b_null_count.sum()/len(b_null_count)).rename('Data_Percentage'))\n",
    "\n",
    "fig, axat = plt.subplots(1,2, figsize=(20,5))\n",
    "ax_b = round(b_null_count.iloc[-1,:],2).sort_values(ascending=False).plot(kind='bar',\n",
    "rot=90,color=['lightcoral'],ax=axat[0])\n",
    "for container in ax_b.containers:\n",
    "    ax_b.bar_label(container,color='black',label_type='center',weight='bold',rotation=90)\n",
    "ax_b.set_title('Percentage of Non-Null for Attributes', fontsize=20, fontweight='bold')\n",
    "\n",
    "\n",
    "# Number of True\n",
    "\n",
    "attributes_check = bus[att_list1]\n",
    "attributes_check.replace({False:0,True:1,'False':0,'True':1},inplace=True)\n",
    "\n",
    "#ax_at = attributes_check.sum(axis=0).sort_values(ascending=False)[0:20].plot(kind='bar',\n",
    "#rot=90,color=['lightcoral'],ax=axat[1])\n",
    "#for container in ax_at.containers:\n",
    "#    ax_at.bar_label(container,color='black',label_type='center',weight='bold',rotation=90)\n",
    "#ax_at.set_title('Number of True for Attributes', fontsize=20, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "#Attribute True %\n",
    "\n",
    "b_null_df = b_null_count.iloc[-1,:].sort_values(ascending=False).to_frame().reset_index().rename(columns={'index':'attribute'})\n",
    "b_null_df = b_null_df[b_null_df.Data_Percentage > 0.2]\n",
    "att_list = b_null_df.attribute.tolist()\n",
    "\n",
    "att_list.extend(attributes_check.sum(axis=0).sort_values(ascending=False)[0:20].reset_index()['index'].tolist())\n",
    "att_list = set(att_list)\n",
    "\n",
    "\n",
    "\n",
    "att_count = pd.DataFrame()\n",
    "for x,y in enumerate(att_list):\n",
    "    new_df = pd.DataFrame(bus.loc[:,y].value_counts())\n",
    "    att_count = pd.concat([att_count,new_df],axis=1)\n",
    "att_count.reset_index(inplace=True)\n",
    "att_count.loc[3,:] = \"\"\n",
    "for i in range(1,24):\n",
    "    att_count.iloc[3,i] = att_count.iloc[1,i] / np.sum(att_count.iloc[:2,i])\n",
    "\n",
    "\n",
    "att_df = att_count.iloc[3,1:].sort_values(ascending=False).reset_index().rename(columns={'index':'attribute',3:'%'})\n",
    "att_df['%'] = att_df['%'].astype(float)\n",
    "att_df['%'] = round(att_df['%'],2)\n",
    "\n",
    "ax_att = att_df.plot(kind='bar',x='attribute',y='%',color=['lightcoral'],width=0.6,ax=axat[1],xlabel=\"\")\n",
    "for container in ax_att.containers:\n",
    "    ax_att.bar_label(container,color='black',label_type='center',weight='bold',rotation=90)\n",
    "ax_att.set_title('Percentage of True in Attributes', fontsize=20, fontweight='bold')\n",
    "plt.savefig('Per. of Non-Null & True Att.',dpi=300,bbox_inches='tight')\n",
    "#plt.savefig('Attribute_True_%.png',dpi=300,bbox_inches='tight')\n",
    "\n",
    "#Select first 20 Attributes\n",
    "selected_att = att_df.iloc[:20,0].tolist()\n",
    "#Add From Percentage of Non-Null\n",
    "selected_att.extend(att_list)\n",
    "#Add From Number of True\n",
    "selected_att.extend(attributes_check.sum(axis=0).sort_values(ascending=False)[0:20].reset_index()['index'].tolist())\n",
    "selected_att.append('RestaurantsPriceRange2')\n",
    "selected_att.append('is_chain')\n",
    "selected_att = set(selected_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Selected Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(round(bus[selected_att].corr(),2),cmap='RdYlGn',linewidths=0.2,square=False)\n",
    "plt.title('Correlation Matrix for Selected Attributes', fontsize=20, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_phy_att = ['Alcohol',\n",
    " 'Ambience_casual',\n",
    " 'BikeParking',\n",
    " 'BusinessAcceptsCreditCards',\n",
    " 'BusinessParking_lot',\n",
    " 'Caters',\n",
    " 'GFM_breakfast',\n",
    " 'GFM_dinner',\n",
    " 'GFM_lunch',\n",
    " 'GoodForKids',\n",
    " 'HasTV',\n",
    " 'is_chain',\n",
    " 'OutdoorSeating',\n",
    " 'RestaurantsDelivery',\n",
    " 'RestaurantsGoodForGroups',\n",
    " 'RestaurantsPriceRange2',\n",
    " 'RestaurantsReservations',\n",
    " 'RestaurantsTableService',\n",
    " 'RestaurantsTakeOut',\n",
    " 'WheelchairAccessible',\n",
    " 'WiFi',\n",
    " 'ch_freq']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(round(bus[selected_phy_att].corr(),2),cmap='RdYlGn',linewidths=0.2,square=False)\n",
    "plt.title('Correlation Matrix for Physical Attributes', fontsize=20, fontweight='bold')\n",
    "plt.savefig('Correlation Matrix for Physical Attributes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex for Physical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(bus[selected_phy_att].describe(),3).T.iloc[:,[0,1,2,3,7]].to_latex().replace(\"\\\\\\n\", \"\\\\ \\hline\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Numerical Analysis DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_attributes = [ 'stars', 'is_chain', 'b_num', 'haversine_dist', 'pc_income', 'population']\n",
    "desc_attributes.extend(selected_att)\n",
    "\n",
    "bus_desc = bus[desc_attributes]\n",
    "bus_desc.replace({False:0,True:1},inplace=True)\n",
    "\n",
    "bus_desc = bus_desc.merge(review_avg,on='b_num')\n",
    "bus_desc = round(bus_desc,3)\n",
    "bus_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C - Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Files before Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Weather Data\n",
    "\n",
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Set time period\n",
    "start = datetime(2015, 2, 2)\n",
    "end = datetime(2015, 4, 1)\n",
    "weather_data = pd.DataFrame()\n",
    "\n",
    "for i in range(len(bus)):\n",
    "    loc = Point(bus.iloc[i,6],bus.iloc[i,7])\n",
    "    # Get Daily data\n",
    "    w_data = Daily(loc, start, end)\n",
    "    w_data = w_data.fetch()\n",
    "    w_data.reset_index(inplace=True)\n",
    "    w_data['b_num'] = bus.loc[i,'b_num']\n",
    "    weather_data = weather_data.append(w_data)\n",
    "    weather_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "weather_data.rename(columns={'time':'date'},inplace=True)\n",
    "weather_data_full = weather_data.copy()\n",
    "#weather_data_full.to_csv('weather_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_full = pd.read_csv('weather_data_full.csv')\n",
    "weather_data_full.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\"\"\"\n",
    "weather_data_full.date  = weather_data_full.date.apply(lambda x: pd.to_datetime(x))\n",
    "weather_data_full.date =  weather_data_full.date.apply(lambda x: datetime.date(x))\n",
    "\"\"\"\n",
    "weather_data_full.date = pd.to_datetime(weather_data_full.date)\n",
    "weather_data_full.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_full.iloc[:,[1,4]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Businesses without Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_num_no_weather = np.setdiff1d(bus['b_num'].unique(), weather_data_full['b_num'].unique()).tolist()\n",
    "b_num_no_weather\n",
    "#No Weather Data for 9 Businesses, Drop Them\n",
    "#bus.drop(bus[bus['b_num'].isin(b_num_no_weather)].index,inplace=True)\n",
    "#bus.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create b_id_NZ_df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Unique Businesses dropped to 1790, so save them as b_id_NZ_df_4.csv\n",
    "\n",
    "#Add Unique values of business_id in review_nz_2015 to a new dataframe\n",
    "b_id_nz_5_df = pd.DataFrame(list(bus['b_num'].unique()))\n",
    "b_id_nz_5_df.rename(columns={0:'b_num'},inplace=True)\n",
    "\n",
    "b_id_nz_5_df = b_id_nz_5_df.merge(bus[['business_id','b_num']],on='b_num',how='left')\n",
    "#Drop Duplicates\n",
    "b_id_nz_5_df.drop_duplicates(inplace=True)\n",
    "b_id_nz_5_df.reset_index(drop=True,inplace=True)\n",
    "#b_id_nz_5_df.to_csv('b_id_NZ_df_5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bus Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_pred = bus_desc.copy()\n",
    "bus_pred.drop('business_id',inplace=True,axis=1)\n",
    "bus_pred.replace({np.NaN:0,'None':0},inplace=True)\n",
    "bus_pred = bus_pred.merge(b_id_nz_5_df,on='b_num',how='right')\n",
    "bus_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frac Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This part used on sample analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 30 Random Date, 25% of Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datelist\n",
    "\n",
    "datelist_random = pd.date_range(start=np.datetime64('2015-01-01'), end= np.datetime64('2015-12-31'),freq='D').to_list()\n",
    "random_datelist = datelist_random\n",
    "random_datelist.sort()\n",
    "for i in range(len(random_datelist)):\n",
    "    random_datelist[i] = datetime.date(random_datelist[i])\n",
    "\n",
    "#Get 25% of Businesses\n",
    "bus_frac = bus_pred.sample(frac=1)\n",
    "bus_frac= bus_frac.sort_values('b_num')\n",
    "bus_frac.reset_index(drop=True,inplace=True)\n",
    "\n",
    "date_df = pd.DataFrame(random_datelist)\n",
    "date_df.rename(columns={0:'date'},inplace=True)\n",
    "date_df = date_df.iloc[np.repeat(np.arange(len(date_df)), len(bus_frac))]\n",
    "date_df['date'] = pd.to_datetime(date_df['date']).dt.normalize()\n",
    "date_df.sort_values('date',ascending=True).reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Crete Data File\n",
    "data_col2 = ['date','b_num']\n",
    "data = pd.DataFrame(columns=data_col2)\n",
    "\n",
    "data['b_num'] = bus_frac['b_num']\n",
    "\n",
    "data1 = data.copy()\n",
    "for i in range(len(random_datelist)-1):\n",
    "    data = data.append(data1)\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "date_df.reset_index(drop=True,inplace=True)\n",
    "data['date'] = date_df['date']\n",
    "data['date'] = pd.to_datetime(data['date']).dt.normalize()\n",
    "\n",
    "data = data.sort_values(['date','b_num'])\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "data = pd.merge(data,bus_frac,on='b_num',how='left')\n",
    "\n",
    "#Add Weather\n",
    "data = pd.merge(data,weather_data_full[['b_num','date','tavg','prcp']], on=['b_num','date'],how='left').sort_values(['b_num','date'])\n",
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_daily = pd.read_csv('review_daily.csv')\n",
    "\n",
    "review_daily_col = review_daily.columns.tolist()\n",
    "for i in range(1, len(review_daily.columns)):\n",
    "    review_daily_col[i] = pd.to_datetime(review_daily_col[i])\n",
    "    review_daily_col[i] = datetime.date(review_daily_col[i])\n",
    "review_daily.columns = review_daily_col\n",
    "\n",
    "review_daily = review_daily.merge(b_id_nz_5_df,on='business_id',how='right')\n",
    "\n",
    "review_daily.sort_values(['b_num'],inplace=True,ignore_index=True)\n",
    "\n",
    "#Frac\n",
    "review_daily_frac = review_daily[review_daily.b_num.isin(bus_frac.b_num.unique().tolist())]\n",
    "review_daily_frac.reset_index(drop=True,inplace=True)\n",
    "\n",
    "first_line = np.arange(len(data))\n",
    "# 0 to 13350\n",
    "second_line = np.repeat(np.arange(0,len(bus_frac)), len(random_datelist))\n",
    "# 0 to 445, each 30 times\n",
    "third_line = np.tile(random_datelist, len(bus_frac))\n",
    "data['review'] = \"\"\n",
    "for x,y,z in zip(first_line,second_line,third_line):\n",
    "    data.iloc[x,-1] = review_daily_frac.loc[y,z]\n",
    "\n",
    "data.review.replace(np.NaN,0,inplace=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i,'review'] > 0:\n",
    "        data.loc[i,'review'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_daily = pd.read_csv('tip_daily.csv')\n",
    "\n",
    "tip_daily_col = tip_daily.columns.tolist()\n",
    "for i in range(1, len(tip_daily.columns)):\n",
    "    tip_daily_col[i] = pd.to_datetime(tip_daily_col[i])\n",
    "    tip_daily_col[i] = datetime.date(tip_daily_col[i])\n",
    "tip_daily.columns = tip_daily_col\n",
    "\n",
    "tip_daily = tip_daily.merge(b_id_nz_5_df,on='business_id',how='right')\n",
    "\n",
    "tip_daily.sort_values(['b_num'],inplace=True,ignore_index=True)\n",
    "\n",
    "#Frac\n",
    "tip_daily_frac = tip_daily[tip_daily.b_num.isin(bus_frac.b_num.unique().tolist())]\n",
    "tip_daily_frac.reset_index(drop=True,inplace=True)\n",
    "\n",
    "first_line = np.arange(len(data))\n",
    "# 0 to 13350\n",
    "second_line = np.repeat(np.arange(0,len(bus_frac)), len(random_datelist))\n",
    "# 0 to 445, each 30 times\n",
    "third_line = np.tile(random_datelist, len(bus_frac))\n",
    "data['tips'] = \"\"\n",
    "for x,y,z in zip(first_line,second_line,third_line):\n",
    "    data.iloc[x,-1] = tip_daily_frac.loc[y,z]\n",
    "\n",
    "\n",
    "data.tips.replace(np.NaN,0,inplace=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i,'tips'] > 0:\n",
    "        data.loc[i,'tips'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_t_2015 = pd.read_csv('check_t_2015.csv')\n",
    "check_tt = check_t_2015.T\n",
    "check_tt.iloc[0,:] = pd.to_datetime(check_tt.iloc[0,:])\n",
    "for i in range(0,len(check_tt.columns)):\n",
    "    check_tt.iloc[0,i] = check_tt.iloc[0,i].date()\n",
    "check_tt.columns += 1\n",
    "\n",
    "check_tt.reset_index(inplace=True)\n",
    "check_tt.rename(columns={'index':'business_id'},inplace=True)\n",
    "\n",
    "check_tt.columns = check_tt.iloc[0,:]\n",
    "check_tt = check_tt[1:]\n",
    "check_tt.rename(columns={'Date':'business_id'},inplace=True)\n",
    "\n",
    "check_tt = check_tt.merge(b_id_nz_5_df,on='business_id',how='right')\n",
    "check_tt.drop('business_id',inplace=True,axis=1)\n",
    "check_tt.sort_values(['b_num'],inplace=True,ignore_index=True)\n",
    "\n",
    "check_tt_frac = check_tt[check_tt.b_num.isin(bus_frac.b_num.unique().tolist())]\n",
    "check_tt_frac.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "first_line = np.arange(len(data))\n",
    "# 0 to 13350\n",
    "second_line = np.repeat(np.arange(0,len(bus_frac)), len(random_datelist))\n",
    "# 0 to 445, each 30 times\n",
    "third_line = np.tile(random_datelist, len(bus_frac))\n",
    "data['check'] = \"\"\n",
    "for x,y,z in zip(first_line,second_line,third_line):\n",
    "    data.iloc[x,-1] = check_tt_frac.loc[y,z]\n",
    "    \n",
    "data.check.replace(np.NaN,0,inplace=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i,'check'] > 0:\n",
    "        data.loc[i,'check'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datelist\n",
    "\n",
    "random_datelist = pd.date_range(start=np.datetime64('2015-01-01'), end= np.datetime64('2015-12-31'),freq='D').to_list()\n",
    "random_datelist.sort()\n",
    "for i in range(len(random_datelist)):\n",
    "    random_datelist[i] = datetime.date(random_datelist[i])\n",
    "\n",
    "date_df = pd.DataFrame(random_datelist)\n",
    "date_df.rename(columns={0:'date'},inplace=True)\n",
    "date_df = date_df.iloc[np.repeat(np.arange(len(date_df)), len(bus_frac))]\n",
    "date_df['date'] = pd.to_datetime(date_df['date']).dt.normalize()\n",
    "date_df.sort_values('date',ascending=True)\n",
    "date_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "bus_frac = bus_pred\n",
    "bus_frac= bus_frac.sort_values('b_num')\n",
    "bus_frac.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Crete Data File\n",
    "\n",
    "\n",
    "data_1 = pd.DataFrame(np.tile(bus_frac['b_num'], len(random_datelist)))\n",
    "data_1.rename(columns={0:'b_num'},inplace=True)\n",
    "\n",
    "data = pd.DataFrame(columns=['date','b_num'])\n",
    "data['b_num'] = data_1['b_num']\n",
    "data['date'] = date_df['date']\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date']).dt.normalize()\n",
    "\n",
    "data = data.sort_values(['date','b_num'])\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "data = pd.merge(data,bus_frac,on='b_num',how='left')\n",
    "\n",
    "#Add Weather\n",
    "data = pd.merge(data,weather_data_full[['b_num','date','tavg','prcp']], on=['b_num','date'],how='left').sort_values(['b_num','date'])\n",
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Daily Review, Tips, Checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_daily = pd.read_csv('review_daily.csv')\n",
    "\n",
    "review_daily_col = review_daily.columns.tolist()\n",
    "for i in range(1, len(review_daily.columns)):\n",
    "    review_daily_col[i] = pd.to_datetime(review_daily_col[i])\n",
    "    review_daily_col[i] = datetime.date(review_daily_col[i])\n",
    "review_daily.columns = review_daily_col\n",
    "\n",
    "review_daily = review_daily.merge(b_id_nz_5_df,on='business_id',how='right')\n",
    "\n",
    "review_daily.sort_values(['b_num'],inplace=True,ignore_index=True)\n",
    "\n",
    "review_daily.drop(['business_id','b_num'],inplace=True,axis=1)\n",
    "\n",
    "review_list = []\n",
    "for i in range(len(review_daily)):\n",
    "    review_list.extend(review_daily.iloc[i,:])\n",
    "data['review'] = review_list\n",
    "\n",
    "\n",
    "#Tips\n",
    "tip_daily = pd.read_csv('tip_daily.csv')\n",
    "\n",
    "tip_daily_col = tip_daily.columns.tolist()\n",
    "for i in range(1, len(tip_daily.columns)):\n",
    "    tip_daily_col[i] = pd.to_datetime(tip_daily_col[i])\n",
    "    tip_daily_col[i] = datetime.date(tip_daily_col[i])\n",
    "tip_daily.columns = tip_daily_col\n",
    "\n",
    "tip_daily = tip_daily.merge(b_id_nz_5_df,on='business_id',how='right')\n",
    "\n",
    "tip_daily.sort_values(['b_num'],inplace=True,ignore_index=True)\n",
    "\n",
    "tip_daily.drop(['business_id','b_num'],inplace=True,axis=1)\n",
    "tip_list = []\n",
    "for i in range(len(tip_daily)):\n",
    "    tip_list.extend(tip_daily.iloc[i,:])\n",
    "data['tips'] = tip_list\n",
    "\n",
    "\n",
    "#Check\n",
    "\n",
    "check_t_2015 = pd.read_csv('check_t_2015.csv')\n",
    "check_tt = check_t_2015.T\n",
    "check_tt.iloc[0,:] = pd.to_datetime(check_tt.iloc[0,:])\n",
    "for i in range(0,len(check_tt.columns)):\n",
    "    check_tt.iloc[0,i] = check_tt.iloc[0,i].date()\n",
    "check_tt.columns += 1\n",
    "\n",
    "check_tt.reset_index(inplace=True)\n",
    "check_tt.rename(columns={'index':'business_id'},inplace=True)\n",
    "\n",
    "check_tt.columns = check_tt.iloc[0,:]\n",
    "check_tt = check_tt[1:]\n",
    "check_tt.rename(columns={'Date':'business_id'},inplace=True)\n",
    "\n",
    "check_tt = check_tt.merge(b_id_nz_5_df,on='business_id',how='right')\n",
    "check_tt.drop('business_id',inplace=True,axis=1)\n",
    "check_tt.sort_values(['b_num'],inplace=True,ignore_index=True)\n",
    "\n",
    "check_tt.drop('b_num',inplace=True,axis=1)\n",
    "check_list = []\n",
    "for i in range(len(check_tt)):\n",
    "    check_list.extend(check_tt.iloc[i,:])\n",
    "data['check'] = check_list\n",
    "\n",
    "\n",
    "\n",
    "data.review.replace(np.NaN,0,inplace=True)\n",
    "data.tips.replace(np.NaN,0,inplace=True)\n",
    "data.check.replace(np.NaN,0,inplace=True)\n",
    "\n",
    "data.review = data.review.astype(bool).astype(int)\n",
    "data.tips = data.tips.astype(bool).astype(int)\n",
    "data.check = data.check.astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Data Without Prcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Business w/out Prcp\n",
    "drop_weather = data.loc[:,'b_num'][data['prcp'].isnull()].unique()\n",
    "#62\n",
    "data = data.drop(data[data['b_num'].isin(drop_weather)].index)\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corr Table for Whole Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(data.corr(),cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.date = pd.to_datetime(data.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax0 = data.iloc[:,[0,-1,-5]].groupby(data.date).mean().reset_index().plot(kind='scatter',y=['check'],x=['date'])\n",
    "ax0 = data.set_index('date').iloc[:,[-1,-5]].groupby(pd.Grouper(freq='W')).mean().reset_index().plot(\n",
    "    kind='scatter',y='check',x='date',color='black',figsize=(12,7))\n",
    "ax0.set_ylabel('Check-in Freq',color='black',fontsize=18,fontweight='bold')\n",
    "ax0.set_xlabel('Date',fontsize=12,fontweight='bold')\n",
    "ax0.set_title('Weekly Check-in Percentage and Average Temperature',fontsize=12,fontweight='bold')\n",
    "ax0.legend(['Check'],loc='upper left',fontsize=12,frameon=True)\n",
    "\n",
    "ax1 = ax0.twinx()\n",
    "ax1 = data.set_index('date').iloc[:,[0,-1,-5]].groupby(pd.Grouper(freq='W')).mean().reset_index().plot(\n",
    "    kind='scatter',y='tavg',x='date',ax=ax1,color='red')\n",
    "ax1.set_ylabel('Tavg',color='red',fontsize=15,fontweight='bold')\n",
    "ax1.legend(['Tavg'],loc='upper right',fontsize=12,frameon=True)\n",
    "\n",
    "plt.savefig('Weekly_Checkin&Temp.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Min Max Scaling for Necessary Columns in data\n",
    "data.drop('business_id',axis=1,inplace=True)\n",
    "data.drop('ch_freq',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min Max Scaling\n",
    "scaled_columns = ['stars', 'RestaurantsPriceRange2','haversine_dist',\n",
    "'pc_income', 'population', 'rev_count','r_avg_cool','r_avg_useful','r_avg_stars',\n",
    "'r_avg_funny','u_avg_rev_count','u_avg_friend','u_avg_fans','u_avg_true_star','u_avg_avg_star','u_avg_yelping_month',\n",
    "'u_review_count_2015','M/F Ratio','tavg','prcp']\n",
    "\n",
    "#Min Max Scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "for i in scaled_columns:\n",
    "    data.loc[:,i] = scaler.fit_transform(data.loc[:,i].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D - ML <a name='p4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_to_predict.csv')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation HeatMap for All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr= data.corr()\n",
    "corr = round(corr,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_corr = corr['check'].sort_values(ascending=True)[39:49].plot(kind='barh',figsize=(5,5),color='black')\n",
    "ax_corr.set_xlabel('Correlation',fontsize=12,fontweight='bold')\n",
    "ax_corr.set_title('Positively Correlated Features with Check-in',fontsize=12,fontweight='bold')\n",
    "for container in ax_corr.containers:\n",
    "    ax_corr.bar_label(container, fontsize=12, fontweight='bold', color='white', label_type='center')\n",
    "plt.savefig('Positive_Correlation_Checkin.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_corr2 = corr['check'].sort_values(ascending=True)[:10].plot(kind='barh',figsize=(5,5),color='black')\n",
    "ax_corr2.set_xlabel('Correlation',fontsize=12,fontweight='bold')\n",
    "ax_corr2.set_title('Negatively Correlated Features with Check-in',fontsize=12,fontweight='bold')\n",
    "for container in ax_corr2.containers:\n",
    "    ax_corr2.bar_label(container, fontsize=12, fontweight='bold', color='white',label_type='center')\n",
    "plt.savefig('Negative_Correlation_Checkin.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = data.copy()\n",
    "len(data_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train Test Dataset\n",
    "\n",
    "X = data.drop('check',axis=1)\n",
    "X = X.iloc[:,2:]\n",
    "y = data['check']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df = pd.DataFrame(columns=['Model','param','Accuracy','R2','MSE','ROC','Time','all_results','Predict_Proba','type'])\n",
    "\n",
    "actual = ['Actual',\"\",\"\",\"\",\"\",\"\",\"\",\n",
    "np.array(y_test.reset_index()['check']),\n",
    "\"\",\"\"]\n",
    "results_df.loc[len(results_df),:] = actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ml Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do K-nearest Neighbors for Check Column in data\n",
    "knn_results = []\n",
    "for i in [10,20,50,100]:\n",
    "#Perform Knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    start_time = time.time()\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    proba = knn.predict_proba(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Evaluate the model\n",
    "    #print('-------',i,'-------')\n",
    "    #print('KNN Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    #print('KNN ROC AUC: ', roc_auc_score(y_test, y_pred))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    results_df.loc[len(results_df),:] = [f'KNN_{20}',\n",
    "    i,\n",
    "    round(accuracy_score(y_test, y_pred),4),\n",
    "    round(r2_score(y_test, y_pred),4),\n",
    "    round(mean_squared_error(y_test, y_pred),4),\n",
    "    round(roc_auc_score(y_test, y_pred),4),\n",
    "    round(end_time-start_time,4),\n",
    "    y_pred,\n",
    "    proba[:,1],\n",
    "    'KNN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Linear Regression on dataframe data\n",
    "lr_results = []\n",
    "#Perform Linear Regression on dataframe data, column check\n",
    "lr = LinearRegression()\n",
    "start_time = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "end_time = time.time()\n",
    "results_df.loc[len(results_df),:] = ['Linear Regression',\n",
    "\"\",\n",
    "round(r2_score(y_test, y_pred),4),\n",
    "round(mean_squared_error(y_test, y_pred),4),\n",
    "round(roc_auc_score(y_test, y_pred),4),\n",
    "round(end_time-start_time,4),\n",
    "y_pred,\n",
    "'LR']\n",
    "#Show the Results of Linear Regression\n",
    "print('Linear Regression Mean Squared Error: ', mean_squared_error(y_test, y_pred))\n",
    "print('Linear Regression R2 Score: ', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Naive Bayes on dataframe data, column check\n",
    "#var_smoothing, default 1e-09 try until 5\n",
    "nb = GaussianNB(var_smoothing=1)\n",
    "start_time = time.time()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "proba = np.around(nb.predict_proba(X_test)[:,1],4)\n",
    "end_time = time.time()\n",
    "\n",
    "#Show the Results of Naive Bayes\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "results_df.loc[len(results_df),:] = ['Naive Bayes',\n",
    "1,\n",
    "round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "round(r2_score(y_test, y_pred),4),\n",
    "round(mean_squared_error(y_test, y_pred),4),\n",
    "round(roc_auc_score(y_test, y_pred),4),\n",
    "round(end_time-start_time,4),\n",
    "y_pred,\n",
    "proba,\n",
    "'NB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform SVM on dataframe data, column check\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "#Show the Results of SVM\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "results_df.loc[len(results_df),:] = ['SVM',\n",
    "round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "round(r2_score(y_test, y_pred),4),\n",
    "round(mean_squared_error(y_test, y_pred),4),\n",
    "round(roc_auc_score(y_test, y_pred),4),\n",
    "round(end_time-start_time,4),\n",
    "y_pred,\n",
    "'SVM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform ANN on dataframe data, column check\n",
    "for solv in ['adam','lbfgs']:\n",
    "    for layer in [100,200,500]:        \n",
    "        ann = MLPClassifier(solver=solv, alpha=1e-5, hidden_layer_sizes=(layer, 1), random_state=1)\n",
    "        start_time = time.time()\n",
    "        ann.fit(X_train, y_train)\n",
    "        y_pred = ann.predict(X_test)\n",
    "        proba = np.around(ann.predict_proba(X_test)[:,1],4)\n",
    "        end_time = time.time()\n",
    "        #Show the Results of ANN\n",
    "        print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        #print(\"Confusion Matrix:\",confusion_matrix(y_test, y_pred))\n",
    "        #print(\"Classification Report:\",classification_report(y_test, y_pred))\n",
    "        print(\"ROC AUC:\",roc_auc_score(y_test, y_pred))\n",
    "        results_df.loc[len(results_df),:] = ['ANN',\n",
    "        str(solv)+'_'+str(layer),\n",
    "        round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "        round(r2_score(y_test, y_pred),4),\n",
    "        round(mean_squared_error(y_test, y_pred),4),\n",
    "        round(roc_auc_score(y_test, y_pred),4),\n",
    "        round(end_time-start_time,4),\n",
    "        y_pred,\n",
    "        proba,\n",
    "        'ANN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Random Forest on dataframe data, column check\n",
    "#n_estimator 100,200,500, max_depth 0,20\n",
    "for est in [100,200,500]:\n",
    "    for depth in [5,20]:\n",
    "            rf = RandomForestClassifier(n_estimators=est, random_state=42,max_depth=depth)\n",
    "            start_time = time.time()\n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_test)\n",
    "            proba = np.around(rf.predict_proba(X_test)[:,1],4)\n",
    "            end_time = time.time()\n",
    "            #Evaluate the Results of Random Forest Classifier\n",
    "            print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            results_df.loc[len(results_df),:] = ['RF',\n",
    "            str(est)+'_'+str(depth),\n",
    "            round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "            round(r2_score(y_test, y_pred),4),\n",
    "            round(mean_squared_error(y_test, y_pred),4),\n",
    "            round(roc_auc_score(y_test, y_pred),4),\n",
    "            round(end_time-start_time,4),\n",
    "            y_pred,\n",
    "            proba,\n",
    "            'RF']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, random_state=42,max_depth=5)\n",
    "start_time = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "proba = np.around(rf.predict_proba(X_test)[:,1],4)\n",
    "end_time = time.time()\n",
    "#Evaluate the Results of Random Forest Classifier\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "rf_features = round(pd.DataFrame(X_train.columns,rf.feature_importances_).reset_index().rename(columns={'index':'importance',\n",
    "            0:'feature'}).sort_values('importance',ascending=False)[:10],3).sort_values('importance',\n",
    "            ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = rf_features.plot(kind='barh',x='feature',y='importance',color='black',figsize=(5,5))\n",
    "for container in ax0.containers:\n",
    "    ax0.bar_label(container, label_type='center',fontsize=10, color='white', fontweight='bold')\n",
    "ax0.set_title('Random Forest Feature Importance')\n",
    "plt.savefig('RF_Feature_Importance_black.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform XG Boost on dataframe data, column check\n",
    "\n",
    "#max_depth, def6, try 3,10, eta def 0.3 try 0.1 0.3 0.5\n",
    "for depth in [3,6,10]:\n",
    "    for etax in [0.1,0.3,0.5]:\n",
    "        xgb = XGBClassifier(eta=etax,max_depth=depth)\n",
    "        start_time = time.time()\n",
    "        xgb.fit(X_train, y_train)\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        proba = np.around(xgb.predict_proba(X_test)[:,1],4)\n",
    "        end_time = time.time()\n",
    "        #Evaluate the Results of XG Boost Classifier\n",
    "        print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\",metrics.roc_auc_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        ax_gb = pd.DataFrame(X_train.columns,xgb.feature_importances_).reset_index().rename(columns={'index':'importance',\n",
    "        0:'feature'}).sort_values('importance',ascending=False)[:20].sort_values('importance',\n",
    "        ascending=True).plot(kind='barh',x='feature',y='importance')\n",
    "        ax_gb.title.set_text('XG Boost Feature Importance')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        results_df.loc[len(results_df),:] = ['XG Boost',\n",
    "        str(depth)+'_'+str(etax),\n",
    "        round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "        round(r2_score(y_test, y_pred),4),\n",
    "        round(mean_squared_error(y_test, y_pred),4),\n",
    "        round(metrics.roc_auc_score(y_test, y_pred),4),\n",
    "        round(end_time-start_time,4),\n",
    "        y_pred,\n",
    "        proba,\n",
    "        'XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(eta=0.3,max_depth=6)\n",
    "start_time = time.time()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "proba = np.around(xgb.predict_proba(X_test)[:,1],4)\n",
    "end_time = time.time()\n",
    "#Evaluate the Results of XG Boost Classifier\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\",metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_gb = round(pd.DataFrame(X_train.columns,xgb.feature_importances_).reset_index().rename(columns={'index':'importance',\n",
    "0:'feature'}).sort_values('importance',ascending=False)[:10],3).sort_values('importance',\n",
    "ascending=True).plot(kind='barh',x='feature',y='importance',figsize=(5,5),color='black')\n",
    "for container in ax_gb.containers:\n",
    "    ax_gb.bar_label(container, label_type='center',fontsize=10, color='white', fontweight='bold')\n",
    "ax_gb.title.set_text('XG Boost Feature Importance')\n",
    "#Change background color to white\n",
    "ax_gb.set_facecolor('white')\n",
    "plt.savefig('XG Boost Feature Importance_black.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Imp. Plot for RF & XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_gb = round(pd.DataFrame(X_train.columns,xgb.feature_importances_).reset_index().rename(columns={'index':'importance',\n",
    "0:'feature'}).sort_values('importance',ascending=False)[:10],3).sort_values('importance',\n",
    "ascending=True).plot(kind='barh',x='feature',y='importance',color='lightcoral')\n",
    "for container in ax_gb.containers:\n",
    "    ax_gb.bar_label(container, label_type='center',fontsize=10, color='black', fontweight='bold')\n",
    "ax_gb.set_title('XG Boost Feature Importance',fontsize=12,fontweight='bold')\n",
    "plt.savefig('XG Boost Feature Importance.png',dpi=300,bbox_inches='tight')\n",
    "\n",
    "#Increase the white space between the subplots\n",
    "ax0 = rf_features.plot(kind='barh',x='feature',y='importance',color='#145369')\n",
    "for container in ax0.containers:\n",
    "    ax0.bar_label(container, label_type='center',fontsize=10, color='white', fontweight='bold')\n",
    "ax0.set_title('Random Forest Feature Importance',fontsize=12,fontweight='bold')\n",
    "plt.savefig('Random Forest Feature Importance.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 XGBRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform XGBoost Random Forest Classifier on dataframe data, column check\n",
    "#nestimators, 20,50,100, max depth def 6 try 3,6,10\n",
    "\n",
    "for est in [20,50,100]:\n",
    "    for depth in [3,6,10]:\n",
    "        start_time = time.time()\n",
    "        xgbrf = XGBRFClassifier(n_estimators=est, max_depth=depth)\n",
    "        xgbrf.fit(X_train, y_train)\n",
    "        y_pred = xgbrf.predict(X_test)\n",
    "        proba = np.around(xgbrf.predict_proba(X_test)[:,1],4)\n",
    "        end_time = time.time()\n",
    "        #Evaluate the Results of XG Boost Random Forest Classifier\n",
    "        print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\",metrics.roc_auc_score(y_test, y_pred))\n",
    "        pd.DataFrame(X_train.columns,xgbrf.feature_importances_).reset_index().rename(columns={'index':'importance',\n",
    "        0:'feature'}).sort_values('importance',ascending=False)[:20].sort_values('importance',\n",
    "        ascending=True).plot(kind='barh',x='feature',y='importance')\n",
    "        plt.show()\n",
    "        results_df.loc[len(results_df),:] = ['XGBRF',\n",
    "        str(est)+'_'+str(depth),\n",
    "        round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "        round(r2_score(y_test, y_pred),4),\n",
    "        round(mean_squared_error(y_test, y_pred),4),\n",
    "        round(metrics.roc_auc_score(y_test, y_pred),4),\n",
    "        round(end_time-start_time,4),\n",
    "        y_pred,\n",
    "        proba,\n",
    "        'XGBRF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Tree on dataframe data, column check\n",
    "#criterion def gini, try entropy, log_loss\n",
    "for crit in ['gini','entropy']:\n",
    "    tree = DecisionTreeClassifier(criterion=crit)\n",
    "    start_time = time.time()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    proba = np.around(tree.predict_proba(X_test)[:,1],4)\n",
    "    end_time = time.time()\n",
    "    #Evaluate the Results of Decision Tree Classifier\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC AUC:\",metrics.roc_auc_score(y_test, y_pred))\n",
    "    pd.DataFrame(X_train.columns,tree.feature_importances_).reset_index().rename(columns={'index':'importance',\n",
    "    0:'feature'}).sort_values('importance',ascending=False)[:20].sort_values('importance',\n",
    "    ascending=True).plot(kind='barh',x='feature',y='importance')\n",
    "    plt.show()\n",
    "    results_df.loc[len(results_df),:] = ['Decision Tree',\n",
    "    str(crit),\n",
    "    round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "    round(r2_score(y_test, y_pred),4),\n",
    "    round(mean_squared_error(y_test, y_pred),4),\n",
    "    round(metrics.roc_auc_score(y_test, y_pred),4),\n",
    "    round(end_time-start_time,4),\n",
    "    y_pred,\n",
    "    proba,\n",
    "    'DecisionT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#penalty default l2, try l1, elasticnet, max_iter def 100 try 100,250,500\n",
    "for solv in ['newton-cg', 'lbfgs','saga']:\n",
    "    for iter in [100,250,500]:\n",
    "        start_time = time.time()\n",
    "        logreg = LogisticRegression(solver=solv,max_iter=iter,)\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        proba = np.around(logreg.predict_proba(X_test)[:,1],4)\n",
    "        end_time = time.time()\n",
    "        #Evaluate the Results of Logistic Regression\n",
    "        print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\",metrics.roc_auc_score(y_test, y_pred))\n",
    "        \"\"\"\n",
    "        pd.DataFrame(X_train.columns,logreg.coef_).reset_index().rename(columns={'index':'importance',\n",
    "        0:'feature'}).sort_values('importance',ascending=False)[:20].sort_values('importance',\n",
    "        ascending=True).plot(kind='barh',x='feature',y='importance')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        results_df.loc[len(results_df),:] = ['Logistic Regression',\n",
    "        str(solv)+'_'+str(iter),\n",
    "        round(metrics.accuracy_score(y_test, y_pred),4),\n",
    "        round(r2_score(y_test, y_pred),4),\n",
    "        round(mean_squared_error(y_test, y_pred),4),\n",
    "        round(metrics.roc_auc_score(y_test, y_pred),4),\n",
    "        round(end_time-start_time,4),\n",
    "        y_pred,\n",
    "        proba,\n",
    "        'LogReg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('ANN',MLPClassifier()),\n",
    "    ('XGB', XGBClassifier()),\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('XGBRF', XGBRFClassifier()),\n",
    "    ('NB',GaussianNB()),\n",
    "    ('LogReg',LogisticRegression()),\n",
    "    ]\n",
    "stacked = StackingClassifier(\n",
    "    estimators = base_models,\n",
    "    final_estimator = XGBClassifier(),\n",
    "    cv = 5)\n",
    "    \n",
    "start_time = time.time()\n",
    "stacked.fit(X_train, y_train)    \n",
    "stacked_prediction = stacked.predict(X_test)\n",
    "#proba = np.around(stacked.prediction_proba(X_test)[:,1],4)\n",
    "end_time = time.time()\n",
    "stacked_r2 = stacked.score(X_test, y_test)\n",
    "stacked_rmse = mean_squared_error(y_test, stacked_prediction, squared = False)\n",
    "print(\"-------Stacked Ensemble-------\")\n",
    "print(\"Coefficient of determination: {}\".format(stacked_r2))\n",
    "print(\"Root Mean Squared Error: {}\".format(stacked_rmse))\n",
    "print(\"Computation Time: {}\".format(end_time - start_time))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, stacked_prediction))\n",
    "print(\"ROC AUC:\",metrics.roc_auc_score(y_test, stacked_prediction))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "results_df.loc[len(results_df),:] = ['Stacked',\n",
    "\"\",\n",
    "round(metrics.accuracy_score(y_test, stacked_prediction),4),\n",
    "round(r2_score(y_test, stacked_prediction),4),\n",
    "round(mean_squared_error(y_test, stacked_prediction),4),\n",
    "round(metrics.roc_auc_score(y_test, stacked_prediction),4),\n",
    "round(end_time-start_time,4),\n",
    "y_pred,\n",
    "\"\",\n",
    "'Stacked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Dataframe for Parameters\n",
    "param_df = pd.DataFrame(columns=['Model','1st Parameter','2nd Parameter'])\n",
    "#Add all models to Model Column\n",
    "param_df['Model'] = results_df_best['Model']\n",
    "param_df.reset_index(drop=True, inplace=True)\n",
    "param_df.iloc[0,1] = 'Var Smoothing: Default = 1e-09,\\n Tested = [1e-09:1]'\n",
    "print(param_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle('results_df_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_best = results_df.iloc[[0,5,9,17,30,32,36,44,46],:]\n",
    "for i in list([2,3,4,5,6,10,11]):\n",
    "    results_df_best.iloc[:,i] = pd.to_numeric(results_df_best.iloc[:,i],errors='coerce')\n",
    "results_df_best.iloc[8,1] = 'Default'\n",
    "results_df_best = round(results_df_best,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_best.replace('KNN_20','KNN_50',inplace=True)\n",
    "results_df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0 = results_df_best.iloc[:,:].sort_values('Accuracy',ascending=False).plot(\n",
    "    x='Model',y=['Accuracy','Gini Coef.','TDL'],kind='bar',figsize=(18,7),rot=0,sharey=True,\n",
    "    color=['black','red','grey'],width=0.85)\n",
    "for container in ax0.containers:\n",
    "    ax0.bar_label(container, fontsize=12,label_type='center',color='white',fontweight='bold')\n",
    "ax0.set_title('Accuracy Rate, Gini Coefficient and TDL for Models',fontsize=15,fontweight='bold')\n",
    "ax0.set_xticklabels(ax0.get_xticklabels(), fontsize=12, weight='bold')\n",
    "#Add Horizontal Grid\n",
    "ax0.yaxis.grid(True, linestyle='-', which='major', color='grey',alpha=0.5)\n",
    "ax0.set_xlabel('Model',fontsize=15,weight='bold')\n",
    "plt.legend(loc='upper right',  ncol=1, fontsize=12)\n",
    "plt.savefig('Gini Coef., Acc, TDL for Models_2.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('ROC',ascending=False,inplace=True)\n",
    "plt.figure(figsize=(15,7))\n",
    "ax1 = sns.barplot(x='ROC',y='Model',data=results_df,palette='Set1',hue='type',orient='h',dodge=False)\n",
    "for container in ax1.containers:\n",
    "    ax1.bar_label(container, label=container.get_label(), label_type='center', fontsize=10,color='white',weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.iloc[:,:-2].sort_values('Accuracy',ascending=False).to_latex(index=False).replace(\"\\\\\\n\", \"\\\\ \\hline\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini and TDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle('results_df_2.pkl')\n",
    "results_df_copy = results_df.copy()\n",
    "\n",
    "#results_df.reset_index(drop=True,inplace=True)\n",
    "#results_df = results_df.iloc[:-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    array = array.flatten() #all values are treated equally, arrays must be 1d\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array) #values cannot be negative\n",
    "    array += 0.0000001 #values cannot be 0\n",
    "    array = np.sort(array) #values must be sorted\n",
    "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
    "    n = array.shape[0]#number of array elements\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient\n",
    "\n",
    "results_df['Gini Coef.'] = \"\"\n",
    "for i in range(0,len(results_df)-1):\n",
    "    results_df.iloc[i,10] = round(gini(results_df.iloc[i,8]),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_2 = pd.DataFrame(columns=['Model','Proba','DecileGroup'])\n",
    "\n",
    "results_df_best = results_df.iloc[[0,5,9,17,30,32,36,44,46],:]\n",
    "\n",
    "#To Create TDL\n",
    "results_tdl = pd.DataFrame()\n",
    "for i in range(0,len(results_df)):\n",
    "    print(i)\n",
    "    try:\n",
    "        if results_df.loc[i,'Model'] != 'Decision Tree':\n",
    "            results_df_2 = pd.DataFrame(columns=['Model','param','Proba','Result','DecileGroup'])\n",
    "            results_df_2.iloc[:,2] = results_df.iloc[i,8]\n",
    "            results_df_2.iloc[:,0] = results_df.iloc[i,0]\n",
    "            results_df_2.iloc[:,1] = results_df.iloc[i,1]\n",
    "            results_df_2.iloc[:,3] = results_df.iloc[i,7]\n",
    "            results_df_2.iloc[:,4] = pd.qcut(results_df_2.iloc[:,2],10,labels=False)\n",
    "            results_tdl = results_tdl.append(results_df_2)\n",
    "    except:\n",
    "        print('Error')\n",
    "        pass\n",
    "\n",
    "results_tdl_2 = results_tdl.groupby(['Model','param','DecileGroup'])['Result'].mean().unstack().reset_index()\n",
    "\n",
    "results_tdl_3 = results_tdl_2.iloc[:,[0,1,11]].merge(results_df[['Model','param','Accuracy']],on=['Model','param'],how='inner')\n",
    "\n",
    "results_tdl_3['TDL'] = results_tdl_3.iloc[:,2] / results_tdl_3.iloc[:,3]\n",
    "#Merge with the results_df\n",
    "results_df_2 = results_df.merge(results_tdl_3[['Model','param','TDL']],on=['Model','param'],how='left')\n",
    "results_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_best.rename(columns={'param':'Parameter'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df_best.iloc[:,[0,1,2,3,4,5,10,11]].to_latex(index=False).replace(\"\\\\\\n\", \"\\\\ \\hline\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61c344a2eba4c315fc9ba157587f4e468595b78019c0b39a54b5fb17b16ab6d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
